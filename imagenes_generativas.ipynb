{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c802f3",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2ec7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "from hydra import initialize, compose\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfac406",
   "metadata": {},
   "source": [
    "### Se toman las 4 carpetas (cable, capsule, screw y transistor) y se separa su información de testing training y se juntan en un solo dataset, igualmente guardando las etiquetas y se setea el tamaño de cada imagen en 128x128 como se indica en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "DATASETS = ['cable', 'capsule', 'screw', 'transistor']\n",
    "BASE_PATH = Path('../TareaAutoEncoders')\n",
    "OUTPUT_PATH = BASE_PATH / 'DATASET_128x128'\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "# Crear estructura de salida (carpetas planas)\n",
    "for split in ['train', 'test', 'ground_truth']:\n",
    "    (OUTPUT_PATH / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_and_save(src_path: Path, dest_dir: Path, prefix: str, is_mask=False):\n",
    "    \"\"\"Lee, redimensiona y guarda. Si is_mask usa INTER_NEAREST.\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(src_path), cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            print(f\"No se pudo leer: {src_path}\")\n",
    "            return False\n",
    "        interp = cv2.INTER_NEAREST if is_mask else cv2.INTER_AREA\n",
    "        resized = cv2.resize(img, IMAGE_SIZE, interpolation=interp)\n",
    "        dest = dest_dir / f\"{prefix}_{src_path.stem}{src_path.suffix}\"\n",
    "        cv2.imwrite(str(dest), resized)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error con {src_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Procesar datasets\n",
    "for dataset in DATASETS:\n",
    "    base = BASE_PATH / dataset\n",
    "\n",
    "    # train -> normalmente sólo 'good' en estos datasets\n",
    "    train_dir = base / 'train'\n",
    "    if train_dir.exists():\n",
    "        for cls in train_dir.iterdir():\n",
    "            if not cls.is_dir(): \n",
    "                continue\n",
    "            for img in cls.glob('*.*'):\n",
    "                prefix = f\"{dataset}_train_{cls.name}\"\n",
    "                process_and_save(img, OUTPUT_PATH / 'train', prefix, is_mask=False)\n",
    "\n",
    "    # test -> incluir good y defectos\n",
    "    test_dir = base / 'test'\n",
    "    if test_dir.exists():\n",
    "        for cls in test_dir.iterdir():\n",
    "            if not cls.is_dir():\n",
    "                continue\n",
    "            for img in cls.glob('*.*'):\n",
    "                prefix = f\"{dataset}_test_{cls.name}\"\n",
    "                process_and_save(img, OUTPUT_PATH / 'test', prefix, is_mask=False)\n",
    "\n",
    "    # ground_truth -> máscaras (usar nearest)\n",
    "    gt_dir = base / 'ground_truth'\n",
    "    if gt_dir.exists():\n",
    "        for cls in gt_dir.iterdir():\n",
    "            if not cls.is_dir():\n",
    "                continue\n",
    "            for img in cls.glob('*.*'):\n",
    "                prefix = f\"{dataset}_gt_{cls.name}\"\n",
    "                process_and_save(img, OUTPUT_PATH / 'ground_truth', prefix, is_mask=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb3c98",
   "metadata": {},
   "source": [
    "## Configuración de los archivos Hydra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d30650",
   "metadata": {},
   "source": [
    "### ¿Por qué?\n",
    "La librería Hydra permite establecer las configuraciones que va a tener la ejecución del modelo, lo cual permite una forma eficaz de cambiar los parámetros con los que será entrenado el mismo sin tener que hacer variaciones en los parámetros del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123bc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio conf/ creado\n"
     ]
    }
   ],
   "source": [
    "# Crear estructura base\n",
    "conf_path = Path(\"conf\")\n",
    "conf_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Directorio conf/ creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209cb794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdirectorios creados:\n",
      "   - conf/model/\n",
      "   - conf/trainer/\n",
      "   - conf/logger/\n",
      "   - conf/loss/\n",
      "   - conf/optimizer/\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Crear carpetas necesarias\n",
    "subdirs = [\"model\", \"trainer\", \"logger\", \"loss\", \"optimizer\"]\n",
    "for subdir in subdirs:\n",
    "    (conf_path / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Subdirectorios creados:\")\n",
    "for subdir in subdirs:\n",
    "    print(f\"   - conf/{subdir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c5c90",
   "metadata": {},
   "source": [
    "### Solicitudes del enunciado:\n",
    "\n",
    "A como estaba solicitado en el enunciado, se crean diferentes sets de configuraciones .yaml que serán guardadas en la carpeta conf para su posterior uso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ae3b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variaciones de modelo creadas:\n",
      "   - autoencoder_latent_small (128)\n",
      "   - autoenceoder_latent_large (1024)\n"
     ]
    }
   ],
   "source": [
    "# Celda: Crear variaciones de configuración para experimentos\n",
    "# Variación 1: Latent dim pequeño\n",
    "latent_small_yaml = \"\"\"name: autoencoder_latent_small\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256]\n",
    "latent_dim: 128\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_latent_small.yaml\", \"w\") as f:\n",
    "    f.write(latent_small_yaml)\n",
    "\n",
    "# Variación 2: Latent dim grande\n",
    "latent_large_yaml = \"\"\"name: autoencoder_latent_large\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256]\n",
    "latent_dim: 1024\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_latent_large.yaml\", \"w\") as f:\n",
    "    f.write(latent_large_yaml)\n",
    "\n",
    "print(\"Variaciones de modelo creadas:\")\n",
    "print(\"   - autoencoder_latent_small (128)\")\n",
    "print(\"   - autoenceoder_latent_large (1024)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53282c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/config.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Crear conf/config.yaml (configuración principal)\n",
    "config_yaml = \"\"\"defaults:\n",
    "  - model: autoencoder\n",
    "  - trainer: default\n",
    "  - logger: wandb\n",
    "  - loss: l2\n",
    "  - optimizer: adam_mid\n",
    "\n",
    "seed: 42\n",
    "\n",
    "data:\n",
    "  data_dir: 'DATASET_128x128'\n",
    "  image_size: 128\n",
    "  batch_size: 32\n",
    "  num_workers: 0\n",
    "  validation_split: 0.15\n",
    "  test_split: 0.15\n",
    "\n",
    "callbacks:\n",
    "  monitor: \"val/loss\"\n",
    "  mode: \"min\"\n",
    "  filename: \"{epoch:02d}-{val/loss:.4f}\"\n",
    "  save_top_k: 3\n",
    "\n",
    "experiment:\n",
    "  name: \"default_experiment\"\n",
    "  description: \"Default autoencoder experiment\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/config.yaml\", \"w\") as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(\"conf/config.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e13d661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/autoencoder.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Crear modelos - conf/model/autoencoder.yaml\n",
    "autoencoder_yaml = \"\"\"name: autoencoder\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256]\n",
    "latent_dim: 512\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder.yaml\", \"w\") as f:\n",
    "    f.write(autoencoder_yaml)\n",
    "\n",
    "print(\"conf/model/autoencoder.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5d4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/unet.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Crear modelos - conf/model/unet.yaml\n",
    "unet_yaml = \"\"\"name: unet\n",
    "in_channels: 3\n",
    "base_channels: 32\n",
    "depth: 4\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/unet.yaml\", \"w\") as f:\n",
    "    f.write(unet_yaml)\n",
    "\n",
    "print(\"conf/model/unet.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4fef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/autoencoder_small.yaml creado (latent_dim: 128)\n"
     ]
    }
   ],
   "source": [
    "# Variaciones de autoencoder con latent_dim pequeño\n",
    "autoencoder_small_yaml = \"\"\"name: autoencoder_small\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128]\n",
    "latent_dim: 128\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_small.yaml\", \"w\") as f:\n",
    "    f.write(autoencoder_small_yaml)\n",
    "\n",
    "print(\"conf/model/autoencoder_small.yaml creado (latent_dim: 128)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03c99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/autoencoder_large.yaml creado (latent_dim: 1024)\n"
     ]
    }
   ],
   "source": [
    "# Variaciones de autoencoder con latent_dim grande\n",
    "autoencoder_large_yaml = \"\"\"name: autoencoder_large\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256, 512]\n",
    "latent_dim: 1024\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_large.yaml\", \"w\") as f:\n",
    "    f.write(autoencoder_large_yaml)\n",
    "\n",
    "print(\"conf/model/autoencoder_large.yaml creado (latent_dim: 1024)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3cdcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/l1.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Funciones de pérdida - L1\n",
    "l1_yaml = \"\"\"name: l1\n",
    "type: L1Loss\n",
    "weight: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/l1.yaml\", \"w\") as f:\n",
    "    f.write(l1_yaml)\n",
    "\n",
    "print(\"conf/loss/l1.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae19c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/l2.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Funciones de pérdida - L2 (MSE)\n",
    "l2_yaml = \"\"\"name: l2\n",
    "type: MSELoss\n",
    "weight: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/l2.yaml\", \"w\") as f:\n",
    "    f.write(l2_yaml)\n",
    "\n",
    "print(\"conf/loss/l2.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818cd767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/ssim.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Funciones de pérdida - SSIM\n",
    "ssim_yaml = \"\"\"name: ssim\n",
    "type: SSIMLoss\n",
    "weight: 1.0\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/ssim.yaml\", \"w\") as f:\n",
    "    f.write(ssim_yaml)\n",
    "\n",
    "print(\"conf/loss/ssim.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e24c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/ssim_l1.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Funciones de pérdida - SSIM + L1\n",
    "ssim_l1_yaml = \"\"\"name: ssim_l1\n",
    "type: SSIMLoss_L1\n",
    "weight_ssim: 0.5\n",
    "weight_l1: 0.5\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/ssim_l1.yaml\", \"w\") as f:\n",
    "    f.write(ssim_l1_yaml)\n",
    "\n",
    "print(\"conf/loss/ssim_l1.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f27b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/trainer/default.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Trainer - conf/trainer/default.yaml\n",
    "trainer_yaml = \"\"\"max_epochs: 20\n",
    "gpus: 1\n",
    "precision: 32\n",
    "deterministic: true\n",
    "check_val_every_n_epoch: 1\n",
    "log_every_n_steps: 10\n",
    "enable_model_summary: true\n",
    "gradient_clip_val: 0.0\n",
    "enable_progress_bar: true\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/trainer/default.yaml\", \"w\") as f:\n",
    "    f.write(trainer_yaml)\n",
    "\n",
    "print(\"conf/trainer/default.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ea0d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/logger/wandb.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Logger - conf/logger/wandb.yaml\n",
    "wandb_yaml = \"\"\"project: ae_experiments\n",
    "entity: null\n",
    "log_model: false\n",
    "offline: false\n",
    "tags: []\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/logger/wandb.yaml\", \"w\") as f:\n",
    "    f.write(wandb_yaml)\n",
    "\n",
    "print(\"conf/logger/wandb.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01369ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/optimizer/adam_low.yaml creado (lr: 1e-4)\n"
     ]
    }
   ],
   "source": [
    "# Optimizer - Adam con LR bajo\n",
    "adam_low_yaml = \"\"\"name: adam_low\n",
    "type: Adam\n",
    "lr: 1e-4\n",
    "weight_decay: 0.0\n",
    "betas: [0.9, 0.999]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/optimizer/adam_low.yaml\", \"w\") as f:\n",
    "    f.write(adam_low_yaml)\n",
    "\n",
    "print(\"conf/optimizer/adam_low.yaml creado (lr: 1e-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eafba72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/optimizer/adam_mid.yaml creado (lr: 1e-3)\n"
     ]
    }
   ],
   "source": [
    "# Optimizer - Adam con LR medio\n",
    "adam_mid_yaml = \"\"\"name: adam_mid\n",
    "type: Adam\n",
    "lr: 1e-3\n",
    "weight_decay: 0.0\n",
    "betas: [0.9, 0.999]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/optimizer/adam_mid.yaml\", \"w\") as f:\n",
    "    f.write(adam_mid_yaml)\n",
    "\n",
    "print(\"conf/optimizer/adam_mid.yaml creado (lr: 1e-3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f83b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/optimizer/adam_high.yaml creado (lr: 5e-3)\n"
     ]
    }
   ],
   "source": [
    "# Optimizer - Adam con LR alto\n",
    "adam_high_yaml = \"\"\"name: adam_high\n",
    "type: Adam\n",
    "lr: 5e-3\n",
    "weight_decay: 1e-5\n",
    "betas: [0.9, 0.999]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/optimizer/adam_high.yaml\", \"w\") as f:\n",
    "    f.write(adam_high_yaml)\n",
    "\n",
    "print(\"conf/optimizer/adam_high.yaml creado (lr: 5e-3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8291b",
   "metadata": {},
   "source": [
    "## Definición del DataModule y modelo base (Autoencoder clásico con Hydra + PyTorch Lightning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff540c",
   "metadata": {},
   "source": [
    "### Definición del Dataset para DATASET_128x128\n",
    "\n",
    "En esta sección definimos una clase `MVTecDataset` basada en `torch.utils.data.Dataset`\n",
    "que carga las imágenes ya preprocesadas a tamaño **128×128**.\n",
    "\n",
    "Las imágenes se cargan en formato RGB y se convierten a tensores normalizados en \\[0, 1].\n",
    "Este dataset se utilizará dentro del `LightningDataModule` para separar train/val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3402efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        split_dir = self.root_dir / split\n",
    "        exts = (\".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "        self.image_paths = sorted(\n",
    "            [p for p in split_dir.glob(\"*.*\") if p.suffix.lower() in exts],\n",
    "            key=lambda p: p.name\n",
    "        )\n",
    "\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(f\"[WARNING] No se encontraron imágenes en {split_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Para autoencoder solo necesitamos la imagen (entrada = salida)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235779d1",
   "metadata": {},
   "source": [
    "### LightningDataModule para MVTec\n",
    "\n",
    "Para estructurar el flujo de datos usando PyTorch Lightning, se define un\n",
    "`LightningDataModule` llamado `MVTecDataModule`.\n",
    "\n",
    "Este módulo:\n",
    "\n",
    "- Recibe los hiperparámetros desde la configuración (`cfg.data`):\n",
    "  - `data_dir`, `batch_size`, `num_workers`, `validation_split`.\n",
    "- Construye el `Dataset` de entrenamiento completo y lo separa en:\n",
    "  - subconjunto de **train**\n",
    "  - subconjunto de **validation** (usando `validation_split`).\n",
    "- Crea el `Dataset` de **test**.\n",
    "- Expone los `DataLoader`:\n",
    "  - `train_dataloader()`\n",
    "  - `val_dataloader()`\n",
    "  - `test_dataloader()`\n",
    "\n",
    "De esta forma, el mismo `DataModule` se reutiliza para todos los modelos y\n",
    "experimentos (distintas funciones de pérdida, arquitecturas, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a39e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        val_split=0.15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "\n",
    "        # Transformación básica: convertir a tensor en [0,1]\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Dataset completo de entrenamiento (después se divide en train/val)\n",
    "        full_train = MVTecDataset(\n",
    "            root_dir=self.data_dir,\n",
    "            split=\"train\",\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "        n_total = len(full_train)\n",
    "        n_val = int(self.val_split * n_total)\n",
    "        n_train = n_total - n_val\n",
    "\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(\n",
    "            full_train,\n",
    "            [n_train, n_val],\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "\n",
    "        # Dataset de test\n",
    "        self.test_set = MVTecDataset(\n",
    "            root_dir=self.data_dir,\n",
    "            split=\"test\",\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdfeab",
   "metadata": {},
   "source": [
    "### Construcción de función de pérdida y optimizador desde Hydra\n",
    "\n",
    "La configuración de la tarea se maneja con Hydra. En particular:\n",
    "\n",
    "- `conf/loss/*.yaml` define el tipo de función de pérdida a usar\n",
    "  (L1, L2, SSIM, SSIM + L1).\n",
    "- `conf/optimizer/*.yaml` define el tipo de optimizador y sus hiperparámetros\n",
    "  (por ejemplo, Adam con distintas tasas de aprendizaje).\n",
    "\n",
    "Para desacoplar el modelo de estas decisiones, se implementan dos funciones:\n",
    "\n",
    "- `build_loss(loss_cfg)`: a partir de `cfg.loss` devuelve un objeto de pérdida\n",
    "  de PyTorch.\n",
    "- `build_optimizer(optimizer_cfg, parameters)`: a partir de `cfg.optimizer`\n",
    "  devuelve una instancia del optimizador apropiado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95a64429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(loss_cfg): #Loss_cfg es el diccionario generado por Hydra a partir de las configuraciones .yaml\n",
    "    \"\"\"\n",
    "    Construye la función de pérdida a partir de cfg.loss.\n",
    "    Por ahora se implementan L1 y L2 (MSE).\n",
    "    \"\"\"\n",
    "    loss_type = loss_cfg.type # Se asignan los valores ajustados a la configuracion\n",
    "\n",
    "    if loss_type == \"L1Loss\":\n",
    "        return nn.L1Loss()\n",
    "    elif loss_type == \"MSELoss\":\n",
    "        return nn.MSELoss()\n",
    "    else:\n",
    "        # Aquí luego se agregarán SSIM y SSIM+L1\n",
    "        raise NotImplementedError(f\"Pérdida '{loss_type}' aún no implementada en este notebook.\")\n",
    "    \n",
    "\n",
    "def build_optimizer(optimizer_cfg, parameters):\n",
    "    \"\"\"\n",
    "    Construye el optimizador a partir de cfg.optimizer.\n",
    "    Actualmente soporta Adam con lr configurable.\n",
    "    \"\"\"\n",
    "    opt_type = optimizer_cfg.type\n",
    "\n",
    "    if opt_type == \"Adam\":\n",
    "        return torch.optim.Adam(\n",
    "            parameters,\n",
    "            lr=optimizer_cfg.lr,\n",
    "            weight_decay=optimizer_cfg.weight_decay,\n",
    "            betas=tuple(optimizer_cfg.betas),\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer '{opt_type}' no implementado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84d0f0",
   "metadata": {},
   "source": [
    "### Modelo base: Autoencoder clásico con PyTorch Lightning\n",
    "\n",
    "En esta sección se define el modelo de **autoencoder clásico** como un\n",
    "`LightningModule` llamado `LitAutoencoder`.\n",
    "\n",
    "Este módulo:\n",
    "\n",
    "- Lee su configuración desde `cfg.model`:\n",
    "  - `in_channels`\n",
    "  - `hidden_dims` (lista de canales intermedios)\n",
    "  - `latent_dim`\n",
    "- Construye un **encoder** convolucional que reduce la resolución de la imagen.\n",
    "- Aplica capas totalmente conectadas para:\n",
    "  - Proyectar la salida del encoder a un espacio latente de dimensión `latent_dim`.\n",
    "  - Reconstruir desde el espacio latente a la forma intermedia del decoder.\n",
    "- Construye un **decoder** con convoluciones transpuestas para recuperar\n",
    "  una imagen de tamaño 128×128 y 3 canales.\n",
    "- Utiliza la función de pérdida definida en `cfg.loss`.\n",
    "- Utiliza el optimizador definido en `cfg.optimizer`.\n",
    "\n",
    "Este modelo será el primero en usarse para los experimentos de la tarea\n",
    "(con distintas funciones de pérdida). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "805bbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, model_cfg, loss_cfg, optimizer_cfg, image_size=128):\n",
    "        super().__init__()\n",
    "        # Guardamos la config de modelo como hyperparams (para reproducibilidad)\n",
    "        self.save_hyperparameters(OmegaConf.to_container(model_cfg, resolve=True))\n",
    "\n",
    "        self.model_cfg = model_cfg\n",
    "        self.loss_cfg = loss_cfg\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.image_size = image_size\n",
    "\n",
    "        in_channels = model_cfg.in_channels\n",
    "        hidden_dims = list(model_cfg.hidden_dims)\n",
    "        latent_dim = model_cfg.latent_dim\n",
    "\n",
    "        # Encoder: secuencia de convoluciones con stride 2\n",
    "        modules = []\n",
    "        channels = in_channels\n",
    "        size = image_size\n",
    "        for h in hidden_dims:\n",
    "            modules.append(nn.Conv2d(channels, h, kernel_size=3, stride=2, padding=1))\n",
    "            modules.append(nn.ReLU())\n",
    "            channels = h\n",
    "            size = size // 2  # cada conv con stride 2 reduce la mitad\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.enc_out_channels = channels\n",
    "        self.enc_out_size = size\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(channels * size * size, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, channels * size * size)\n",
    "\n",
    "        # Decoder: conv transpuestas para volver a 3x128x128\n",
    "        modules = []\n",
    "        hidden_dims_rev = list(hidden_dims[::-1])\n",
    "\n",
    "        for i in range(len(hidden_dims_rev) - 1):\n",
    "            modules.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    hidden_dims_rev[i],\n",
    "                    hidden_dims_rev[i + 1],\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            modules.append(nn.ReLU())\n",
    "\n",
    "        modules.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims_rev[-1],\n",
    "                in_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            )\n",
    "        )\n",
    "        modules.append(nn.Sigmoid())  # salida en [0,1]\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        # Pérdida\n",
    "        self.criterion = build_loss(loss_cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        x_flat = self.flatten(x_enc)\n",
    "        z = self.fc_mu(x_flat)\n",
    "        x_dec_flat = self.fc_decode(z)\n",
    "        x_dec = x_dec_flat.view(\n",
    "            x.shape[0],\n",
    "            self.enc_out_channels,\n",
    "            self.enc_out_size,\n",
    "            self.enc_out_size,\n",
    "        )\n",
    "        x_hat = self.decoder(x_dec)\n",
    "        return x_hat\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = self.criterion(x_hat, x)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = build_optimizer(self.optimizer_cfg, self.parameters())\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a8978",
   "metadata": {},
   "source": [
    "### Función principal de entrenamiento con Hydra y WandB\n",
    "\n",
    "Finalmente, se define una función `train_autoencoder_with_hydra()` que\n",
    "integra todos los componentes anteriores:\n",
    "\n",
    "1. Inicializa Hydra y carga la configuración desde `conf/config.yaml`.\n",
    "2. Construye el `MVTecDataModule` usando `cfg.data`.\n",
    "3. Construye el `LitAutoencoder` usando:\n",
    "   - `cfg.model` (arquitectura del autoencoder clásico),\n",
    "   - `cfg.loss` (función de pérdida),\n",
    "   - `cfg.optimizer` (optimizador).\n",
    "4. Inicializa un `WandbLogger` con los parámetros de `cfg.logger`.\n",
    "5. Crea un `Trainer` de PyTorch Lightning con los parámetros definidos en `cfg.trainer`.\n",
    "6. Llama a `trainer.fit(model, datamodule=dm)` para entrenar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce1d2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_with_hydra():\n",
    "    \"\"\"\n",
    "    Función de entrenamiento principal.\n",
    "    Usa Hydra para cargar conf/config.yaml y los subarchivos.\n",
    "    \"\"\"\n",
    "    with initialize(config_path=\"conf\", version_base=None):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "\n",
    "    print(\"Configuración cargada:\")\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # DataModule\n",
    "    dm = MVTecDataModule(\n",
    "        data_dir=cfg.data.data_dir,\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        val_split=cfg.data.validation_split,\n",
    "    )\n",
    "\n",
    "    # Modelo (autoencoder clásico)\n",
    "    model = LitAutoencoder(\n",
    "        model_cfg=cfg.model,\n",
    "        loss_cfg=cfg.loss,\n",
    "        optimizer_cfg=cfg.optimizer,\n",
    "        image_size=cfg.data.image_size,\n",
    "    )\n",
    "\n",
    "    # Logger de WandB\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=cfg.logger.project,\n",
    "        entity=cfg.logger.entity,\n",
    "        log_model=cfg.logger.log_model,\n",
    "    )\n",
    "\n",
    "    # Trainer a partir de cfg.trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.trainer.max_epochs,\n",
    "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "        deterministic=cfg.trainer.deterministic,\n",
    "        enable_model_summary=cfg.trainer.enable_model_summary,\n",
    "        enable_progress_bar=cfg.trainer.enable_progress_bar,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c11f34",
   "metadata": {},
   "source": [
    "### Configuraciones de funciones de pérdida (Hydra)\n",
    "\n",
    "Se requiere evaluar distintas funciones de pérdida para el autoencoder:\n",
    "\n",
    "- **L1**  \n",
    "- **L2 (MSE)**  \n",
    "- **SSIM**  \n",
    "- **SSIM + L1**\n",
    "\n",
    "Para permitir cambiar entre estas variantes desde Hydra sin modificar código,\n",
    "se definen cuatro archivos de configuración en `conf/loss/`:\n",
    "\n",
    "- `l1.yaml`\n",
    "- `l2.yaml`\n",
    "- `ssim.yaml`\n",
    "- `ssim_l1.yaml`\n",
    "\n",
    "Cada uno especifica el tipo de pérdida a usar (`type`) y, en el caso de SSIM,\n",
    "algunos hiperparámetros adicionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8083a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos de configuración de pérdidas creados/actualizados en conf/loss/\n"
     ]
    }
   ],
   "source": [
    "Path(\"conf/loss\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "l1_yaml = \"\"\"type: L1Loss\n",
    "name: \"L1\"\n",
    "\"\"\"\n",
    "\n",
    "l2_yaml = \"\"\"type: MSELoss\n",
    "name: \"L2\"\n",
    "\"\"\"\n",
    "\n",
    "ssim_yaml = \"\"\"type: SSIM\n",
    "name: \"SSIM\"\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "\"\"\"\n",
    "\n",
    "ssim_l1_yaml = \"\"\"type: SSIM_L1\n",
    "name: \"SSIM+L1\"\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "l1_weight: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/l1.yaml\", \"w\") as f:\n",
    "    f.write(l1_yaml)\n",
    "\n",
    "with open(\"conf/loss/l2.yaml\", \"w\") as f:\n",
    "    f.write(l2_yaml)\n",
    "\n",
    "with open(\"conf/loss/ssim.yaml\", \"w\") as f:\n",
    "    f.write(ssim_yaml)\n",
    "\n",
    "with open(\"conf/loss/ssim_l1.yaml\", \"w\") as f:\n",
    "    f.write(ssim_l1_yaml)\n",
    "\n",
    "print(\"Archivos de configuración de pérdidas creados/actualizados en conf/loss/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70f880",
   "metadata": {},
   "source": [
    "### Implementación de la pérdida SSIM\n",
    "\n",
    "La métrica **Structural Similarity Index (SSIM)** mide la similitud estructural\n",
    "entre dos imágenes. A diferencia de L1/L2, que comparan píxel a píxel,\n",
    "SSIM toma en cuenta:\n",
    "\n",
    "- luminancia,\n",
    "- contraste,\n",
    "- estructura local.\n",
    "\n",
    "Para usar SSIM como pérdida, se suele minimizar `1 - SSIM(x, y)`, donde `x` es\n",
    "la imagen original y `y` la reconstrucción del autoencoder.\n",
    "\n",
    "A continuación se define una implementación en PyTorch que:\n",
    "\n",
    "- Convierte la fórmula de SSIM a operaciones de convolución 2D con un kernel\n",
    "  gaussiano.\n",
    "- Calcula SSIM de forma local y luego promedia el resultado.\n",
    "- Devuelve `1 - SSIM` como valor de pérdida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e81108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size=11,\n",
    "        sigma=1.5,\n",
    "        data_range=1.0,\n",
    "        channel=3,\n",
    "        K1=0.01,\n",
    "        K2=0.03,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementación de SSIM como pérdida: loss = 1 - SSIM.\n",
    "        Asume imágenes en rango [0, data_range] y 3 canales por defecto.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.sigma = sigma\n",
    "        self.data_range = data_range\n",
    "        self.channel = channel\n",
    "        self.K1 = K1\n",
    "        self.K2 = K2\n",
    "\n",
    "        self.register_buffer(\"window\", self._create_window(window_size, sigma, channel))\n",
    "\n",
    "    def _gaussian(self, window_size, sigma):\n",
    "        gauss = torch.tensor(\n",
    "            [\n",
    "                (-(x - window_size // 2) ** 2) / float(2 * sigma**2)\n",
    "                for x in range(window_size)\n",
    "            ]\n",
    "        )\n",
    "        gauss = torch.exp(gauss)\n",
    "        return gauss / gauss.sum()\n",
    "\n",
    "    def _create_window(self, window_size, sigma, channel):\n",
    "        _1d_window = self._gaussian(window_size, sigma).unsqueeze(1)\n",
    "        _2d_window = _1d_window @ _1d_window.t()  # producto externo\n",
    "        _2d_window = _2d_window.float().unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "        window = _2d_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x, y: tensores [B, C, H, W] en rango [0, data_range]\n",
    "        Devuelve 1 - SSIM promedio en el batch.\n",
    "        \"\"\"\n",
    "        if x.size(1) != self.channel or y.size(1) != self.channel:\n",
    "            # Simplemente adaptamos el canal si es distinto (por si acaso)\n",
    "            self.channel = x.size(1)\n",
    "            self.window = self._create_window(self.window_size, self.sigma, self.channel).to(x.device)\n",
    "\n",
    "        # Constantes de SSIM\n",
    "        C1 = (self.K1 * self.data_range) ** 2\n",
    "        C2 = (self.K2 * self.data_range) ** 2\n",
    "\n",
    "        # Media local\n",
    "        mu_x = torch.nn.functional.conv2d(\n",
    "            x, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        )\n",
    "        mu_y = torch.nn.functional.conv2d(\n",
    "            y, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        )\n",
    "\n",
    "        mu_x2 = mu_x * mu_x\n",
    "        mu_y2 = mu_y * mu_y\n",
    "        mu_xy = mu_x * mu_y\n",
    "\n",
    "        # Varianzas y covarianza\n",
    "        sigma_x2 = torch.nn.functional.conv2d(\n",
    "            x * x, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        ) - mu_x2\n",
    "        sigma_y2 = torch.nn.functional.conv2d(\n",
    "            y * y, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        ) - mu_y2\n",
    "        sigma_xy = torch.nn.functional.conv2d(\n",
    "            x * y, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        ) - mu_xy\n",
    "\n",
    "        # Fórmula de SSIM\n",
    "        num = (2 * mu_xy + C1) * (2 * sigma_xy + C2)\n",
    "        den = (mu_x2 + mu_y2 + C1) * (sigma_x2 + sigma_y2 + C2)\n",
    "\n",
    "        ssim_map = num / (den + 1e-8)\n",
    "        ssim = ssim_map.mean()\n",
    "\n",
    "        # Pérdida = 1 - SSIM promedio\n",
    "        loss = 1 - ssim\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e06b0a",
   "metadata": {},
   "source": [
    "### Actualización de `build_loss` para incluir SSIM y SSIM+L1\n",
    "\n",
    "Con la clase `SSIMLoss` definida, se extiende la función `build_loss` para\n",
    "reconocer cuatro tipos de pérdida:\n",
    "\n",
    "- `L1Loss`  → L1 estándar.\n",
    "- `MSELoss` → L2 (MSE).\n",
    "- `SSIM`    → `1 - SSIM(x, y)`.\n",
    "- `SSIM_L1` → combinación lineal de SSIM y L1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "259180f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(loss_cfg):\n",
    "    \"\"\"\n",
    "    Construye la función de pérdida a partir de cfg.loss.\n",
    "\n",
    "    Soporta:\n",
    "      - L1Loss\n",
    "      - MSELoss\n",
    "      - SSIM\n",
    "      - SSIM_L1 (combinación SSIM + L1)\n",
    "    \"\"\"\n",
    "    loss_type = loss_cfg.type\n",
    "\n",
    "    if loss_type == \"L1Loss\":\n",
    "        return nn.L1Loss()\n",
    "\n",
    "    elif loss_type == \"MSELoss\":\n",
    "        return nn.MSELoss()\n",
    "\n",
    "    elif loss_type == \"SSIM\":\n",
    "        return SSIMLoss(\n",
    "            window_size=loss_cfg.window_size,\n",
    "            sigma=loss_cfg.sigma,\n",
    "            data_range=loss_cfg.data_range,\n",
    "            channel=3,  # nuestras imágenes son RGB\n",
    "        )\n",
    "\n",
    "    elif loss_type == \"SSIM_L1\":\n",
    "        ssim_loss = SSIMLoss(\n",
    "            window_size=loss_cfg.window_size,\n",
    "            sigma=loss_cfg.sigma,\n",
    "            data_range=loss_cfg.data_range,\n",
    "            channel=3,\n",
    "        )\n",
    "        l1 = nn.L1Loss()\n",
    "        l1_weight = loss_cfg.l1_weight\n",
    "\n",
    "        class SSIML1Combined(nn.Module):\n",
    "            def __init__(self, ssim_loss, l1, l1_weight):\n",
    "                super().__init__()\n",
    "                self.ssim_loss = ssim_loss\n",
    "                self.l1 = l1\n",
    "                self.l1_weight = l1_weight\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                loss_ssim = self.ssim_loss(x, y)        # 1 - SSIM\n",
    "                loss_l1 = self.l1(x, y)\n",
    "                return loss_ssim + self.l1_weight * loss_l1\n",
    "\n",
    "        return SSIML1Combined(ssim_loss, l1, l1_weight)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Pérdida '{loss_type}' aún no implementada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d536a7d",
   "metadata": {},
   "source": [
    "### Configuración del modelo U-Net (Hydra)\n",
    "\n",
    "Se requiere evaluar un autoencoder clásico y un autoencoder tipo **U-Net**.\n",
    "\n",
    "Para permitir seleccionar esta arquitectura desde Hydra sin modificar el código,\n",
    "se crea el archivo `conf/model/unet.yaml`, donde se definen sus parámetros:\n",
    "\n",
    "- `in_channels`: número de canales de entrada (3 para RGB)\n",
    "- `base_channels`: número inicial de filtros del encoder\n",
    "- `depth`: cantidad de niveles de downsampling / upsampling\n",
    "- `latent_dim`: tamaño del cuello (opcional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f99ca031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/unet.yaml creado\n"
     ]
    }
   ],
   "source": [
    "Path(\"conf/model\").mkdir(exist_ok=True)\n",
    "\n",
    "unet_yaml = \"\"\"in_channels: 3\n",
    "base_channels: 32\n",
    "depth: 4\n",
    "latent_dim: 128\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/unet.yaml\", \"w\") as f:\n",
    "    f.write(unet_yaml)\n",
    "\n",
    "print(\"conf/model/unet.yaml creado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd082bc",
   "metadata": {},
   "source": [
    "### Implementación del Autoencoder U-Net\n",
    "\n",
    "Este modelo sigue una estructura típica de U-Net:\n",
    "\n",
    "1. **Encoder**:\n",
    "   - Múltiples niveles de convoluciones + downsampling (stride 2).\n",
    "   - Se almacenan características para las conexiones tipo \"skip\".\n",
    "\n",
    "2. **Bottleneck**:\n",
    "   - Capa completamente conectada para pasar al espacio latente.\n",
    "\n",
    "3. **Decoder**:\n",
    "   - ConvTransposed2D para upsampling simétrico.\n",
    "   - Se concatenan los \"skip connections\" del encoder.\n",
    "\n",
    "Este modelo debe:\n",
    "- Usar la misma función de pérdida configurada en `cfg.loss`\n",
    "- Usar el mismo optimizador configurado en `cfg.optimizer`\n",
    "- Ser llamado desde Hydra con:\n",
    "  `defaults: - model: unet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73f64b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoderBlock(nn.Module):\n",
    "    def _init_(self, in_ch, out_ch):\n",
    "        super()._init_()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNetDecoderBlock(nn.Module):\n",
    "    def _init_(self, in_ch, out_ch):\n",
    "        \"\"\"\n",
    "        in_ch: número de canales de entrada al bloque (antes del upsample)\n",
    "        out_ch: número de canales de salida deseados después del bloque\n",
    "        Este bloque realiza el upsample internamente (ConvTranspose2d) y luego\n",
    "        procesa la concatenación con el skip connection.\n",
    "        \"\"\"\n",
    "        super()._init_()\n",
    "        # up transforma de in_ch -> out_ch (upsample)\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        # luego se concatena con skip (out_ch + skip_ch), asumimos skip_ch == out_ch\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(out_ch * 2, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class LitUNetAutoencoder(pl.LightningModule):\n",
    "    def _init_(self, model_cfg, loss_cfg, optimizer_cfg):\n",
    "        super()._init_()\n",
    "        self.save_hyperparameters(OmegaConf.to_container(model_cfg, resolve=True))\n",
    "\n",
    "        self.model_cfg = model_cfg\n",
    "        self.loss_cfg = loss_cfg\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "\n",
    "        base = model_cfg.base_channels\n",
    "        depth = model_cfg.depth\n",
    "        in_ch = model_cfg.in_channels\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        self.enc_blocks = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        ch = in_ch\n",
    "        channels = []\n",
    "\n",
    "        for d in range(depth):\n",
    "            out_ch = base * (2 ** d)\n",
    "            self.enc_blocks.append(UNetEncoderBlock(ch, out_ch))\n",
    "            channels.append(out_ch)\n",
    "            ch = out_ch\n",
    "            self.downsamples.append(nn.Conv2d(out_ch, out_ch, kernel_size=2, stride=2))\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetEncoderBlock(ch, ch * 2)\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        # Nota: cada UNetDecoderBlock ya hace su propio upsampling internamente.\n",
    "        # Guardamos los bloques decoders en orden inverso.\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        ch = ch * 2  # canales salientes del bottleneck\n",
    "\n",
    "        for d in reversed(range(depth)):\n",
    "            out_ch = base * (2 ** d)\n",
    "            # dec block espera in_ch = ch (canales actuales antes de up), out_ch = canales deseados después\n",
    "            self.dec_blocks.append(UNetDecoderBlock(ch, out_ch))\n",
    "            ch = out_ch\n",
    "\n",
    "        # Output (3 canales)\n",
    "        self.final_conv = nn.Conv2d(base, 3, kernel_size=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = build_loss(loss_cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        out = x\n",
    "\n",
    "        # Encoder\n",
    "        for enc, down in zip(self.enc_blocks, self.downsamples):\n",
    "            out = enc(out)\n",
    "            skips.append(out)\n",
    "            out = down(out)\n",
    "\n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        # Decoder\n",
    "        # IMPORTANTE: aquí NO hacemos up(out) ni cat manualmente porque cada dec_block\n",
    "        # hace el up + concat internamente. Solo llamamos a cada dec_block con (out, skip).\n",
    "        for dec, skip in zip(self.dec_blocks, reversed(skips)):\n",
    "            out = dec(out, skip)\n",
    "\n",
    "        out = self.final_conv(out)\n",
    "        return self.activation(out)\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = self.criterion(x_hat, x)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return build_optimizer(self.optimizer_cfg, self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2f92b",
   "metadata": {},
   "source": [
    "### Función de entrenamiento para soportar Autoencoder y U-Net\n",
    "\n",
    "Se reescribe la función `train_autoencoder_with_hydra()` para:\n",
    "\n",
    "- Cargar la configuración completa desde Hydra.\n",
    "- Crear automáticamente el DataModule.\n",
    "- Instanciar el modelo según `cfg.model`:\n",
    "  - `autoencoder` → `LitAutoencoder`\n",
    "  - `unet` → `LitUNetAutoencoder`\n",
    "- Inicializar WandB.\n",
    "- Crear un Trainer de Lightning.\n",
    "- Entrenar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_with_hydra():\n",
    "    \"\"\"\n",
    "    Versión actualizada: soporta modelos 'autoencoder' y 'unet'.\n",
    "    Utiliza Hydra + Lightning + WandB para ejecutar entrenamientos reproducibles.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Cargar configuración completa desde Hydra\n",
    "    with initialize(config_path=\"conf\", version_base=None):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "\n",
    "    print(\"=========== CONFIGURACIÓN CARGADA ===========\")\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    # 2. Crear DataModule con los parámetros de cfg.data\n",
    "    dm = MVTecDataModule(\n",
    "        data_dir=cfg.data.data_dir,\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        val_split=cfg.data.validation_split,\n",
    "    )\n",
    "\n",
    "    # 3. Instanciar modelo según cfg.model\n",
    "    model_type = cfg.model._target_ if \"_target_\" in cfg.model else None\n",
    "\n",
    "    # Detectar cuál modelo estamos usando\n",
    "    if \"unet\" in cfg.model.__dict__['_content'] or \"unet\" in str(cfg.model):\n",
    "        print(\"Instanciando modelo: U-Net Autoencoder\")\n",
    "        model = LitUNetAutoencoder(\n",
    "            model_cfg=cfg.model,\n",
    "            loss_cfg=cfg.loss,\n",
    "            optimizer_cfg=cfg.optimizer,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Instanciando modelo: Autoencoder clásico\")\n",
    "        model = LitAutoencoder(\n",
    "            model_cfg=cfg.model,\n",
    "            loss_cfg=cfg.loss,\n",
    "            optimizer_cfg=cfg.optimizer,\n",
    "            image_size=cfg.data.image_size,\n",
    "        )\n",
    "\n",
    "    # 4. WandB Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=cfg.logger.project,\n",
    "        entity=cfg.logger.entity,\n",
    "        log_model=cfg.logger.log_model,\n",
    "    )\n",
    "\n",
    "    # 5. Trainer de Lightning con parámetros de Hydra\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.trainer.max_epochs,\n",
    "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "        deterministic=cfg.trainer.deterministic,\n",
    "        enable_model_summary=cfg.trainer.enable_model_summary,\n",
    "        enable_progress_bar=cfg.trainer.enable_progress_bar,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    # 6. Entrenamiento\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    print(\"Entrenamiento finalizado correctamente\")\n",
    "\n",
    "    return model, dm, cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a88f9d",
   "metadata": {},
   "source": [
    "## Experimentación con WandB(Weight and Biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa11078",
   "metadata": {},
   "source": [
    "#### Proposito\n",
    "      Este callback de PyTorch Lightning captura las pérdidas durante el entrenamiento y las visualiza al final\n",
    "\n",
    "#### Hooks utilizados:\n",
    "- on_train_epoch_end: Captura train_loss después de   cada época\n",
    "- on_validation_epoch_end: Captura val_loss después de cada validación\n",
    "- on_fit_end: Crea una gráfica matplotlib y la sube a Weights & Biases\n",
    "\n",
    "\n",
    "#### Manejo de errores: \n",
    "Intenta .item() primero, luego float() para compatibilidad con diferentes versiones de PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57e2fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPlotCallback(pl.Callback):\n",
    "    \"\"\"Callback para graficar train/val loss al final del entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"Guarda el train loss de cada epoch\"\"\"\n",
    "        metrics = trainer.callback_metrics\n",
    "        if 'train_loss' in metrics:\n",
    "            try:\n",
    "                self.train_losses.append(metrics['train_loss'].item())\n",
    "            except Exception:\n",
    "                self.train_losses.append(float(metrics['train_loss']))\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"Guarda el val loss de cada epoch\"\"\"\n",
    "        metrics = trainer.callback_metrics\n",
    "        if 'val_loss' in metrics:\n",
    "            try:\n",
    "                self.val_losses.append(metrics['val_loss'].item())\n",
    "            except Exception:\n",
    "                self.val_losses.append(float(metrics['val_loss']))\n",
    "    \n",
    "    def on_fit_end(self, trainer, pl_module):\n",
    "        \"\"\"Grafica y loguea al final del entrenamiento\"\"\"\n",
    "        if len(self.train_losses) == 0 or len(self.val_losses) == 0:\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        ax.plot(epochs, self.train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "        ax.plot(epochs, self.val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Loss', fontsize=12)\n",
    "        ax.set_title('Training and Validation Loss', fontsize=14)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Loguear en WandB\n",
    "        wandb.log({\"loss_curve\": wandb.Image(fig)})\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bacfc",
   "metadata": {},
   "source": [
    "## Callback para reconstrucción de imágenes\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "- Propósito: Loguea comparaciones lado-a-lado de imágenes originales vs reconstruidas durante validación\n",
    "- Estrategia de selección: Intenta elegir 1 imagen de cada tipo de producto MVTec (cable, capsule, screw, transistor) para mostrar diversidad\n",
    "\n",
    "### Proceso:\n",
    "- Obtiene un batch del validation loader\n",
    "- Intenta seleccionar muestras diversas por categoría de producto\n",
    "- Reconstruye las imágenes usando el modelo (sin gradientes)\n",
    "- Concatena original y reconstrucción horizontalmente\n",
    "- Sube las imágenes a WandB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff470469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageReconstructionLogger(pl.Callback):\n",
    "    \"\"\"Callback para loguear reconstrucciones de imágenes (elige muestras diversas por dataset).\"\"\"\n",
    "\n",
    "    def __init__(self, num_images=4):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.logged_count = 0\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        try:\n",
    "            # Intentar obtener directamente un batch rápido (por compatibilidad)\n",
    "            val_loader = trainer.datamodule.val_dataloader()\n",
    "            batch = next(iter(val_loader))\n",
    "\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch_data = batch[0]\n",
    "            else:\n",
    "                batch_data = batch\n",
    "\n",
    "            # DEBUG: mostrar forma del batch\n",
    "            try:\n",
    "                print(f\"Batch shape: {getattr(batch_data, 'shape', None)}, necesito {self.num_images}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Intentar seleccionar muestras deterministas y diversas (una por dataset si es posible)\n",
    "            try:\n",
    "                val_subset = trainer.datamodule.val_set\n",
    "                if val_subset is not None and hasattr(val_subset, \"dataset\"):\n",
    "                    ds = val_subset.dataset  # MVTecDataset (full dataset antes del split)\n",
    "                    indices = getattr(val_subset, \"indices\", None) or getattr(val_subset, \"_indices\", None)\n",
    "                    if indices is not None and len(indices) > 0:\n",
    "                        # Intentar escoger una muestra por cada dataset (cable, capsule, screw, transistor)\n",
    "                        datasets_names = ['cable', 'capsule', 'screw', 'transistor']\n",
    "                        chosen = []\n",
    "                        seen = set()\n",
    "                        for idx in indices:\n",
    "                            name = ds.image_paths[idx].name.lower()\n",
    "                            for d in datasets_names:\n",
    "                                if d in name and d not in seen:\n",
    "                                    chosen.append(idx)\n",
    "                                    seen.add(d)\n",
    "                                    break\n",
    "                            if len(chosen) >= self.num_images:\n",
    "                                break\n",
    "                        # Si no conseguimos diversidad suficiente, usar los primeros índices del subset\n",
    "                        if len(chosen) < self.num_images:\n",
    "                            chosen = list(indices[: self.num_images])\n",
    "                        sample_idx = chosen[: self.num_images]\n",
    "                        sample_paths = [str(ds.image_paths[i]) for i in sample_idx]\n",
    "                        print(\" Rutas de validación (muestras que se usarán):\", sample_paths)\n",
    "\n",
    "                        # Construir batch_data a partir de los paths seleccionados para evitar confusiones\n",
    "                        batch_tensors = []\n",
    "                        for p in sample_paths:\n",
    "                            img = Image.open(p).convert(\"RGB\")\n",
    "                            if hasattr(ds, \"transform\") and ds.transform is not None:\n",
    "                                t = ds.transform(img)\n",
    "                            else:\n",
    "                                t = transforms.ToTensor()(img)\n",
    "                            batch_tensors.append(t.unsqueeze(0))\n",
    "                        if len(batch_tensors) > 0:\n",
    "                            batch_data = torch.cat(batch_tensors, dim=0).to(pl_module.device)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning selection diversity failed: {e}\")\n",
    "\n",
    "            # Limitar a num_images (por si vino de val_loader)\n",
    "            batch_data = batch_data[: self.num_images].to(pl_module.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reconstructed = pl_module(batch_data)\n",
    "\n",
    "            # Pasar a CPU / numpy y crear visualización\n",
    "            batch_cpu = batch_data.cpu()\n",
    "            reconstructed_cpu = reconstructed.cpu()\n",
    "\n",
    "            original_imgs = batch_cpu.numpy().transpose(0, 2, 3, 1)\n",
    "            reconstructed_imgs = reconstructed_cpu.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "            images_to_log = []\n",
    "            for i in range(len(original_imgs)):\n",
    "                original = original_imgs[i]\n",
    "                recon = reconstructed_imgs[i]\n",
    "                combined = np.concatenate([original, recon], axis=1)\n",
    "                combined = np.clip(combined, 0, 1)\n",
    "                images_to_log.append(wandb.Image(combined, caption=f\"Original vs Reconstructed {i}\"))\n",
    "\n",
    "            if len(images_to_log) > 0:\n",
    "                self.logged_count += len(images_to_log)\n",
    "                print(f\"Logueadas {len(images_to_log)} imágenes. Total: {self.logged_count}\")\n",
    "                wandb.log({\"reconstructions\": images_to_log}, step=trainer.global_step)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en ImageReconstructionLogger: {type(e).__name__}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc89fef",
   "metadata": {},
   "source": [
    "## Visualizador del espacio latente (PCA)\n",
    "\n",
    "### Explicación:\n",
    "\n",
    "Propósito: Visualiza el espacio latente del autoencoder reducido a 2D usando PCA cada 5 épocas\n",
    "\n",
    "### Proceso:\n",
    "\n",
    "- Extrae vectores latentes (z) de ~100 imágenes del validation set\n",
    "- Aplica PCA para reducir dimensionalidad a 2D\n",
    "- Crea scatter plot coloreado secuencialmente\n",
    "- Muestra varianza explicada por cada componente principal\n",
    "\n",
    "Compatibilidad: Verifica que el modelo tenga la estructura esperada (encoder, flatten, fc_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23b431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSpaceVisualizer(pl.Callback):\n",
    "    \"\"\"Callback para visualizar el espacio latente con PCA\"\"\"\n",
    "    \n",
    "    def __init__(self, num_images=100):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "    \n",
    "    def _get_latent(self, pl_module, batch):\n",
    "        \"\"\"Extrae vector latente para Autoencoder y UNet.\"\"\"\n",
    "\n",
    "        # Caso 1: Autoencoder clásico\n",
    "        if hasattr(pl_module, \"encoder\") and hasattr(pl_module, \"fc_mu\"):\n",
    "            x_enc = pl_module.encoder(batch)\n",
    "            x_flat = pl_module.flatten(x_enc)\n",
    "            z = pl_module.fc_mu(x_flat)\n",
    "            return z\n",
    "\n",
    "        # Caso 2: U-Net Autoencoder (usa bottleneck)\n",
    "        if hasattr(pl_module, \"bottleneck\"):\n",
    "            out = batch\n",
    "            # Pasar por encoder UNet\n",
    "            for enc, down in zip(pl_module.enc_blocks, pl_module.downsamples):\n",
    "                out = enc(out)\n",
    "                out = down(out)\n",
    "\n",
    "            # Bottleneck real\n",
    "            out = pl_module.bottleneck(out)\n",
    "\n",
    "            # Aplanar (vector)\n",
    "            z = out.view(out.size(0), -1)\n",
    "            return z\n",
    "\n",
    "        raise AttributeError(\"El modelo no tiene encoder ni bottleneck.\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"Visualiza el espacio latente cada N epochs\"\"\"\n",
    "        if trainer.current_epoch % 5 != 0:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            from sklearn.decomposition import PCA\n",
    "            import matplotlib.pyplot as plt  # ← IMPORTAR AQUI TAMBIÉN\n",
    "        except ImportError as e:\n",
    "            print(f\"Error de importación: {e}\")\n",
    "            return\n",
    "        \n",
    "        val_loader = trainer.datamodule.val_dataloader()\n",
    "        latent_vectors = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                if batch_idx * len(batch) >= self.num_images:\n",
    "                    break\n",
    "                \n",
    "                batch = batch.to(pl_module.device)\n",
    "                \n",
    "                try:\n",
    "                    z = self._get_latent(pl_module, batch)\n",
    "                    latent_vectors.append(z.cpu().numpy())\n",
    "                except AttributeError as e:\n",
    "                    print(f\"⚠️ Modelo incompatible: {e}\")\n",
    "                    return\n",
    "        \n",
    "        if len(latent_vectors) == 0:\n",
    "            print(\"No hay vectores latentes\")\n",
    "            return\n",
    "        \n",
    "        latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "        \n",
    "        print(f\"Aplicando PCA a {len(latent_vectors)} muestras...\")\n",
    "        try:\n",
    "            pca = PCA(n_components=2, random_state=42)\n",
    "            latent_2d = pca.fit_transform(latent_vectors)\n",
    "            \n",
    "            fig = self._create_plot(latent_2d, pca)\n",
    "            wandb.log({\"latent_space\": wandb.Image(fig)}, step=trainer.global_step)\n",
    "            plt.close(fig)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en PCA: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_plot(latent_2d, pca):\n",
    "        \"\"\"Crea figura con PCA\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        scatter = ax.scatter(\n",
    "            latent_2d[:, 0], \n",
    "            latent_2d[:, 1], \n",
    "            c=range(len(latent_2d)), \n",
    "            cmap='viridis', \n",
    "            alpha=0.6, \n",
    "            s=50\n",
    "        )\n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "        ax.set_title('Latent Space Visualization (PCA)')\n",
    "        plt.colorbar(scatter, ax=ax)\n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb398954",
   "metadata": {},
   "source": [
    "### Visualizador de detección de anomalías \n",
    "\n",
    "#### Explicación:\n",
    "Propósito: Evalúa el modelo en el test set cada 5 épocas y visualiza la capacidad de detección de anomalías\n",
    "\n",
    "#### Proceso:\n",
    "- Carga dataset de test (contiene imágenes normales y anómalas)\n",
    "- Selecciona muestras aleatorias y las reconstruye\n",
    "- Calcula MSE (Mean Squared Error) por imagen\n",
    "- Normaliza el error y lo convierte en mapa de calor\n",
    "- Crea visualización de 3 paneles: Original | Reconstrucción | Error Map\n",
    "- Clasifica como \"Good\" o \"Anomaly\" según la mediana del MSE\n",
    "\n",
    "\n",
    "#### Utilidad\n",
    "\n",
    "Permite monitorear si el modelo aprende a detectar anomalías por tener mayor error de reconstrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77e9fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectionVisualizer(pl.Callback):\n",
    "    \"\"\"Callback para visualizar reconstrucciones de imágenes normales y anómalas\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='DATASET_128x128', num_images=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.num_images = num_images\n",
    "        self.test_dataset = None\n",
    "    \n",
    "    def setup(self, trainer, pl_module, stage=None):\n",
    "        \"\"\"Configurar dataset de prueba\"\"\"\n",
    "        if self.test_dataset is None:\n",
    "            self.test_dataset = MVTecDataset(\n",
    "                root_dir=self.data_dir,\n",
    "                split='test',\n",
    "                transform=transforms.ToTensor()\n",
    "            )\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"Visualiza reconstrucciones del set de prueba\"\"\"\n",
    "        if trainer.current_epoch % 5 != 0:\n",
    "            return\n",
    "        \n",
    "        if self.test_dataset is None or len(self.test_dataset) == 0:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Obtener muestras aleatorias\n",
    "            n_samples = min(self.num_images, len(self.test_dataset))\n",
    "            indices = np.random.choice(len(self.test_dataset), n_samples, replace=False)\n",
    "            test_images = []\n",
    "            \n",
    "            for idx in indices:\n",
    "                test_images.append(self.test_dataset[idx])\n",
    "            \n",
    "            test_batch = torch.stack(test_images).to(pl_module.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                reconstructed = pl_module(test_batch)\n",
    "            \n",
    "            # Calcular MSE\n",
    "            mse = torch.mean((test_batch - reconstructed) ** 2, dim=[1, 2, 3])\n",
    "            \n",
    "            # Pasar a CPU\n",
    "            test_batch_cpu = test_batch.cpu()\n",
    "            reconstructed_cpu = reconstructed.cpu()\n",
    "            \n",
    "            # Convertir a numpy\n",
    "            original_imgs = test_batch_cpu.numpy().transpose(0, 2, 3, 1)\n",
    "            reconstructed_imgs = reconstructed_cpu.numpy().transpose(0, 2, 3, 1)\n",
    "            error_maps = mse.cpu().numpy()\n",
    "            \n",
    "            # Crear visualizaciones\n",
    "            images_to_log = []\n",
    "            median_error = np.median(error_maps)\n",
    "            \n",
    "            for i in range(len(original_imgs)):\n",
    "                original = original_imgs[i]\n",
    "                recon = reconstructed_imgs[i]\n",
    "                error = error_maps[i]\n",
    "                \n",
    "                # Normalizar error map a [0, 1]\n",
    "                error_normalized = (error - error_maps.min()) / (error_maps.max() - error_maps.min() + 1e-8)\n",
    "                error_map_3ch = np.stack([error_normalized] * 3, axis=-1)\n",
    "                \n",
    "                # Concatenar paneles\n",
    "                combined = np.concatenate([original, recon, error_map_3ch], axis=1)\n",
    "                combined = np.clip(combined, 0, 1)\n",
    "                \n",
    "                label = \"Good\" if error < median_error else \"Anomaly\"\n",
    "                images_to_log.append(\n",
    "                    wandb.Image(combined, caption=f\"{label} - MSE: {error:.4f}\")\n",
    "                )\n",
    "            \n",
    "            if len(images_to_log) > 0:\n",
    "                wandb.log({\"test_reconstructions\": images_to_log}, step=trainer.global_step)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en AnomalyDetectionVisualizer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8a9d1",
   "metadata": {},
   "source": [
    "### Función principal de experimentación\n",
    "\n",
    "#### Explicación:\n",
    "Propósito: Función orquestadora que ejecuta un experimento completo de entrenamiento\n",
    "\n",
    "#### Pasos:\n",
    "- Detección de hardware: Verifica GPU disponible y muestra información\n",
    "- Inicialización WandB: Crea run para tracking de experimento\n",
    "- Configuración Hydra: Carga configs YAML composables (modelo, loss, optimizer)\n",
    "- DataModule: Instancia el cargador de datos MVTec\n",
    "-Modelo: Selecciona entre autoencoder clásico o U-Net según nombre\n",
    "- Callbacks: Configura ModelCheckpoint, EarlyStopping y visualizadores personalizados\n",
    "- Trainer: Configura PyTorch Lightning con GPU forzado\n",
    "- Entrenamiento: trainer.fit()\n",
    "- Evaluación: trainer.test() en test set\n",
    "- Cleanup: Cierra WandB con finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "598dd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config_name, model_name, loss_name, optimizer_name, run_name):\n",
    "    \"\"\"\n",
    "    Ejecuta un experimento con la configuración especificada\n",
    "    \n",
    "    Args:\n",
    "        config_name: nombre de la configuración base (default: 'config')\n",
    "        model_name: nombre del modelo en conf/model/\n",
    "        loss_name: nombre de la función de pérdida en conf/loss/\n",
    "        optimizer_name: nombre del optimizador en conf/optimizer/\n",
    "        run_name: nombre del run en WandB\n",
    "    \"\"\"\n",
    "    \n",
    "    # VERIFICA GPU DISPONIBLE\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"WARNING: No hay GPU disponible. Se usará CPU.\")\n",
    "        device = \"cpu\"\n",
    "    else:\n",
    "        device = \"cuda\"\n",
    "        print(f\"GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Inicializar WandB\n",
    "    wandb.init(\n",
    "        project=\"ae_experiments\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"model\": model_name,\n",
    "            \"loss\": loss_name,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"device\": device,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Cargar configuración con Hydra\n",
    "        with initialize(config_path=\"conf\", version_base=None):\n",
    "            cfg = compose(\n",
    "                config_name=config_name,\n",
    "                overrides=[\n",
    "                    f\"model={model_name}\",\n",
    "                    f\"loss={loss_name}\",\n",
    "                    f\"optimizer={optimizer_name}\",\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Ejecutando experimento: {run_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(OmegaConf.to_yaml(cfg))\n",
    "        \n",
    "        # Crear DataModule\n",
    "        dm = MVTecDataModule(\n",
    "            data_dir=cfg.data.data_dir,\n",
    "            batch_size=cfg.data.batch_size,\n",
    "            num_workers=cfg.data.num_workers,\n",
    "            val_split=cfg.data.validation_split,\n",
    "        )\n",
    "        \n",
    "        # Instanciar modelo\n",
    "        model_type = model_name.lower()\n",
    "        if \"unet\" in model_type:\n",
    "            print(\"Modelo: U-Net Autoencoder\")\n",
    "            model = LitUNetAutoencoder(\n",
    "                model_cfg=cfg.model,\n",
    "                loss_cfg=cfg.loss,\n",
    "                optimizer_cfg=cfg.optimizer,\n",
    "            )\n",
    "        else:\n",
    "            print(\"Modelo: Autoencoder Clásico\")\n",
    "            model = LitAutoencoder(\n",
    "                model_cfg=cfg.model,\n",
    "                loss_cfg=cfg.loss,\n",
    "                optimizer_cfg=cfg.optimizer,\n",
    "                image_size=cfg.data.image_size,\n",
    "            )\n",
    "        \n",
    "        # Callbacks personalizados\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_top_k=3,\n",
    "                save_last=True,\n",
    "                dirpath=f\"checkpoints/{run_name}\",\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                patience=10,\n",
    "                verbose=True,\n",
    "            ),\n",
    "            ImageReconstructionLogger(num_images=4),\n",
    "            LatentSpaceVisualizer(num_images=100),\n",
    "            AnomalyDetectionVisualizer(num_images=4),\n",
    "        ]\n",
    "        \n",
    "        # Logger de WandB\n",
    "        wandb_logger = WandbLogger(\n",
    "            project=\"ae_experiments\",\n",
    "            name=run_name,\n",
    "            log_model=True,\n",
    "        )\n",
    "        \n",
    "        # TRAINER CON GPU FORZADO\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=cfg.trainer.max_epochs,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=callbacks,\n",
    "            log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "            deterministic=cfg.trainer.deterministic,\n",
    "            enable_model_summary=cfg.trainer.enable_model_summary,\n",
    "            enable_progress_bar=cfg.trainer.enable_progress_bar,\n",
    "            accelerator=\"gpu\", \n",
    "            devices=1,\n",
    "            precision=32,\n",
    "        )\n",
    "        \n",
    "        # Entrenar\n",
    "        print(f\"\\nIniciando entrenamiento en {device.upper()}...\")\n",
    "        trainer.fit(model, datamodule=dm)\n",
    "        \n",
    "        # Evaluar en test set\n",
    "        print(f\"\\nEvaluando en test set...\")\n",
    "        trainer.test(model, datamodule=dm)\n",
    "        \n",
    "        # Loguear métricas finales\n",
    "        wandb.log({\n",
    "            \"final_train_loss\": trainer.callback_metrics.get('train_loss', 0),\n",
    "            \"final_val_loss\": trainer.callback_metrics.get('val_loss', 0),\n",
    "            \"device\": device,\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nExperimento completado: {run_name}\")\n",
    "    \n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3a0dd",
   "metadata": {},
   "source": [
    "### Función para comparación de múltiples experimentos\n",
    "\n",
    "#### Explicación:\n",
    "\n",
    "Propósito: Automatiza la comparación de funciones de pérdida para un modelo específico\n",
    "\n",
    "#### Estrategia: \n",
    "Mantiene constantes todos los hiperparámetros (optimizador, arquitectura) y solo varía la función de pérdida\n",
    "\n",
    "#### Proceso:\n",
    "- Escanea el directorio conf/loss/ para encontrar YAMLs de pérdidas disponibles\n",
    "- Itera sobre cada función de pérdida\n",
    "- Ejecuta run_experiment() con optimizador fijo (adam_mid)\n",
    "- Recopila resultados (éxitos/fallos)\n",
    "- Genera reporte final\n",
    "\n",
    "\n",
    "#### Robustez: \n",
    "Captura excepciones para que un fallo no detenga toda la comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b40c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_all_experiments(model_name, num_loss_functions=4):\n",
    "    \"\"\"\n",
    "    Ejecuta exactamente 4 entrenamientos del mismo modelo\n",
    "    con diferentes funciones de pérdida y MISMOS hiperparámetros\n",
    "    \n",
    "    Args:\n",
    "        model_name: 'autoencoder' o 'unet'\n",
    "        num_loss_functions: número de funciones de pérdida a probar\n",
    "    \"\"\"\n",
    "    \n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Obtener funciones de pérdida disponibles\n",
    "    conf_path = Path(\"conf\")\n",
    "    losses = sorted([f.stem for f in (conf_path / \"loss\").glob(\"*.yaml\")])[:num_loss_functions]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPARACIÓN: {model_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    print(f\"Pérdidas a probar: {losses}\")\n",
    "    print(f\"Optimizador fijo: adam_mid (lr=1e-3)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    results = []\n",
    "    \n",
    "    for i, loss in enumerate(losses, 1):\n",
    "        run_name = f\"{model_name}_{loss}_comparison\"\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(losses)}] Entrenando {model_name} con {loss.upper()}...\")\n",
    "        \n",
    "        try:\n",
    "            run_experiment(\n",
    "                config_name=\"config\",\n",
    "                model_name=model_name,\n",
    "                loss_name=loss,\n",
    "                optimizer_name=\"adam_mid\", \n",
    "                run_name=run_name,\n",
    "            )\n",
    "            successful += 1\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"loss\": loss,\n",
    "                \"status\": \"Exitoso\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            failed += 1\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"loss\": loss,\n",
    "                \"status\": f\"{str(e)[:50]}\"\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPARACIÓN COMPLETADA: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Exitosos: {successful}/{len(losses)}\")\n",
    "    print(f\"Fallidos: {failed}/{len(losses)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d1356",
   "metadata": {},
   "source": [
    "### Script principal\n",
    "\n",
    "#### Explicación:\n",
    "\n",
    "#### Propósito: \n",
    "Script ejecutable que corre todos los experimentos de manera secuencial\n",
    "\n",
    "#### Experimentos ejecutados:\n",
    "\n",
    "- Autoencoder clásico: Prueba 4 funciones de pérdida diferentes\n",
    "- U-Net autoencoder: Prueba las mismas 4 funciones de pérdida\n",
    "\n",
    "Total: 8 entrenamientos completos\n",
    "\n",
    "\n",
    "Output:  Tabla resumen con estado de cada experimento\n",
    "\n",
    "Utilidad: Permite comparar arquitecturas (Autoencoder vs U-Net) y funciones de pérdida en un solo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6b27f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 INICIANDO EXPERIMENTACIÓN CON AUTOENCODERS\n",
      "======================================================================\n",
      "\n",
      "\n",
      "U-NET - Comparación de pérdidas\n",
      "\n",
      "======================================================================\n",
      "🔬 COMPARACIÓN: UNET\n",
      "======================================================================\n",
      "Modelo: unet\n",
      "Pérdidas a probar: ['l1', 'l2', 'ssim', 'ssim_l1']\n",
      "Optimizador fijo: adam_mid (lr=1e-3)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "[1/4] Entrenando unet con L1...\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria: 8.59 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IA\\TareaAutoEncoders\\wandb\\run-20251123_180738-773ovf7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/773ovf7p' target=\"_blank\">unet_l1_comparison</a></strong> to <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/773ovf7p' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/773ovf7p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ejecutando experimento: unet_l1_comparison\n",
      "============================================================\n",
      "model:\n",
      "  in_channels: 3\n",
      "  base_channels: 32\n",
      "  depth: 4\n",
      "  latent_dim: 128\n",
      "trainer:\n",
      "  max_epochs: 20\n",
      "  gpus: 1\n",
      "  precision: 32\n",
      "  deterministic: true\n",
      "  check_val_every_n_epoch: 1\n",
      "  log_every_n_steps: 10\n",
      "  enable_model_summary: true\n",
      "  gradient_clip_val: 0.0\n",
      "  enable_progress_bar: true\n",
      "logger:\n",
      "  project: ae_experiments\n",
      "  entity: null\n",
      "  log_model: false\n",
      "  offline: false\n",
      "  tags: []\n",
      "loss:\n",
      "  type: L1Loss\n",
      "  name: L1\n",
      "optimizer:\n",
      "  name: adam_mid\n",
      "  type: Adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.0\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "seed: 42\n",
      "data:\n",
      "  data_dir: DATASET_128x128\n",
      "  image_size: 128\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  validation_split: 0.15\n",
      "  test_split: 0.15\n",
      "callbacks:\n",
      "  monitor: val/loss\n",
      "  mode: min\n",
      "  filename: '{epoch:02d}-{val/loss:.4f}'\n",
      "  save_top_k: 3\n",
      "experiment:\n",
      "  name: default_experiment\n",
      "  description: Default autoencoder experiment\n",
      "\n",
      "Modelo: U-Net Autoencoder\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unet_l1_comparison</strong> at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/773ovf7p' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/773ovf7p</a><br> View project at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251123_180738-773ovf7p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: _DeviceDtypeModuleMixin.__init__() got an unexpected keyword argument 'model_cfg'\n",
      "\n",
      "[2/4] Entrenando unet con L2...\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria: 8.59 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IA\\TareaAutoEncoders\\wandb\\run-20251123_180740-3lwslnuy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/3lwslnuy' target=\"_blank\">unet_l2_comparison</a></strong> to <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/3lwslnuy' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/3lwslnuy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ejecutando experimento: unet_l2_comparison\n",
      "============================================================\n",
      "model:\n",
      "  in_channels: 3\n",
      "  base_channels: 32\n",
      "  depth: 4\n",
      "  latent_dim: 128\n",
      "trainer:\n",
      "  max_epochs: 20\n",
      "  gpus: 1\n",
      "  precision: 32\n",
      "  deterministic: true\n",
      "  check_val_every_n_epoch: 1\n",
      "  log_every_n_steps: 10\n",
      "  enable_model_summary: true\n",
      "  gradient_clip_val: 0.0\n",
      "  enable_progress_bar: true\n",
      "logger:\n",
      "  project: ae_experiments\n",
      "  entity: null\n",
      "  log_model: false\n",
      "  offline: false\n",
      "  tags: []\n",
      "loss:\n",
      "  type: MSELoss\n",
      "  name: L2\n",
      "optimizer:\n",
      "  name: adam_mid\n",
      "  type: Adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.0\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "seed: 42\n",
      "data:\n",
      "  data_dir: DATASET_128x128\n",
      "  image_size: 128\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  validation_split: 0.15\n",
      "  test_split: 0.15\n",
      "callbacks:\n",
      "  monitor: val/loss\n",
      "  mode: min\n",
      "  filename: '{epoch:02d}-{val/loss:.4f}'\n",
      "  save_top_k: 3\n",
      "experiment:\n",
      "  name: default_experiment\n",
      "  description: Default autoencoder experiment\n",
      "\n",
      "Modelo: U-Net Autoencoder\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unet_l2_comparison</strong> at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/3lwslnuy' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/3lwslnuy</a><br> View project at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251123_180740-3lwslnuy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: _DeviceDtypeModuleMixin.__init__() got an unexpected keyword argument 'model_cfg'\n",
      "\n",
      "[3/4] Entrenando unet con SSIM...\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria: 8.59 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IA\\TareaAutoEncoders\\wandb\\run-20251123_180742-8blh9j47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/8blh9j47' target=\"_blank\">unet_ssim_comparison</a></strong> to <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/8blh9j47' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/8blh9j47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ejecutando experimento: unet_ssim_comparison\n",
      "============================================================\n",
      "model:\n",
      "  in_channels: 3\n",
      "  base_channels: 32\n",
      "  depth: 4\n",
      "  latent_dim: 128\n",
      "trainer:\n",
      "  max_epochs: 20\n",
      "  gpus: 1\n",
      "  precision: 32\n",
      "  deterministic: true\n",
      "  check_val_every_n_epoch: 1\n",
      "  log_every_n_steps: 10\n",
      "  enable_model_summary: true\n",
      "  gradient_clip_val: 0.0\n",
      "  enable_progress_bar: true\n",
      "logger:\n",
      "  project: ae_experiments\n",
      "  entity: null\n",
      "  log_model: false\n",
      "  offline: false\n",
      "  tags: []\n",
      "loss:\n",
      "  type: SSIM\n",
      "  name: SSIM\n",
      "  window_size: 11\n",
      "  sigma: 1.5\n",
      "  data_range: 1.0\n",
      "optimizer:\n",
      "  name: adam_mid\n",
      "  type: Adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.0\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "seed: 42\n",
      "data:\n",
      "  data_dir: DATASET_128x128\n",
      "  image_size: 128\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  validation_split: 0.15\n",
      "  test_split: 0.15\n",
      "callbacks:\n",
      "  monitor: val/loss\n",
      "  mode: min\n",
      "  filename: '{epoch:02d}-{val/loss:.4f}'\n",
      "  save_top_k: 3\n",
      "experiment:\n",
      "  name: default_experiment\n",
      "  description: Default autoencoder experiment\n",
      "\n",
      "Modelo: U-Net Autoencoder\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unet_ssim_comparison</strong> at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/8blh9j47' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/8blh9j47</a><br> View project at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251123_180742-8blh9j47\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: _DeviceDtypeModuleMixin.__init__() got an unexpected keyword argument 'model_cfg'\n",
      "\n",
      "[4/4] Entrenando unet con SSIM_L1...\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria: 8.59 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IA\\TareaAutoEncoders\\wandb\\run-20251123_180744-lrd1zimf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/lrd1zimf' target=\"_blank\">unet_ssim_l1_comparison</a></strong> to <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/lrd1zimf' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/lrd1zimf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ejecutando experimento: unet_ssim_l1_comparison\n",
      "============================================================\n",
      "model:\n",
      "  in_channels: 3\n",
      "  base_channels: 32\n",
      "  depth: 4\n",
      "  latent_dim: 128\n",
      "trainer:\n",
      "  max_epochs: 20\n",
      "  gpus: 1\n",
      "  precision: 32\n",
      "  deterministic: true\n",
      "  check_val_every_n_epoch: 1\n",
      "  log_every_n_steps: 10\n",
      "  enable_model_summary: true\n",
      "  gradient_clip_val: 0.0\n",
      "  enable_progress_bar: true\n",
      "logger:\n",
      "  project: ae_experiments\n",
      "  entity: null\n",
      "  log_model: false\n",
      "  offline: false\n",
      "  tags: []\n",
      "loss:\n",
      "  type: SSIM_L1\n",
      "  name: SSIM+L1\n",
      "  window_size: 11\n",
      "  sigma: 1.5\n",
      "  data_range: 1.0\n",
      "  l1_weight: 0.1\n",
      "optimizer:\n",
      "  name: adam_mid\n",
      "  type: Adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.0\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "seed: 42\n",
      "data:\n",
      "  data_dir: DATASET_128x128\n",
      "  image_size: 128\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  validation_split: 0.15\n",
      "  test_split: 0.15\n",
      "callbacks:\n",
      "  monitor: val/loss\n",
      "  mode: min\n",
      "  filename: '{epoch:02d}-{val/loss:.4f}'\n",
      "  save_top_k: 3\n",
      "experiment:\n",
      "  name: default_experiment\n",
      "  description: Default autoencoder experiment\n",
      "\n",
      "Modelo: U-Net Autoencoder\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unet_ssim_l1_comparison</strong> at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/lrd1zimf' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/lrd1zimf</a><br> View project at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251123_180744-lrd1zimf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: _DeviceDtypeModuleMixin.__init__() got an unexpected keyword argument 'model_cfg'\n",
      "\n",
      "======================================================================\n",
      "COMPARACIÓN COMPLETADA: unet\n",
      "======================================================================\n",
      "Exitosos: 0/4\n",
      "Fallidos: 4/4\n",
      "======================================================================\n",
      "\n",
      "AUTOENCODER - Comparación de pérdidas\n",
      "\n",
      "======================================================================\n",
      "🔬 COMPARACIÓN: AUTOENCODER_SMALL\n",
      "======================================================================\n",
      "Modelo: autoencoder_small\n",
      "Pérdidas a probar: ['l1', 'l2', 'ssim', 'ssim_l1']\n",
      "Optimizador fijo: adam_mid (lr=1e-3)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "[1/4] Entrenando autoencoder_small con L1...\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria: 8.59 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IA\\TareaAutoEncoders\\wandb\\run-20251123_180747-n5q6mee6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/n5q6mee6' target=\"_blank\">autoencoder_small_l1_comparison</a></strong> to <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/n5q6mee6' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/n5q6mee6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ejecutando experimento: autoencoder_small_l1_comparison\n",
      "============================================================\n",
      "model:\n",
      "  name: autoencoder_small\n",
      "  in_channels: 3\n",
      "  hidden_dims:\n",
      "  - 32\n",
      "  - 64\n",
      "  - 128\n",
      "  latent_dim: 128\n",
      "  use_batch_norm: true\n",
      "  dropout_rate: 0.0\n",
      "trainer:\n",
      "  max_epochs: 20\n",
      "  gpus: 1\n",
      "  precision: 32\n",
      "  deterministic: true\n",
      "  check_val_every_n_epoch: 1\n",
      "  log_every_n_steps: 10\n",
      "  enable_model_summary: true\n",
      "  gradient_clip_val: 0.0\n",
      "  enable_progress_bar: true\n",
      "logger:\n",
      "  project: ae_experiments\n",
      "  entity: null\n",
      "  log_model: false\n",
      "  offline: false\n",
      "  tags: []\n",
      "loss:\n",
      "  type: L1Loss\n",
      "  name: L1\n",
      "optimizer:\n",
      "  name: adam_mid\n",
      "  type: Adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.0\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "seed: 42\n",
      "data:\n",
      "  data_dir: DATASET_128x128\n",
      "  image_size: 128\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  validation_split: 0.15\n",
      "  test_split: 0.15\n",
      "callbacks:\n",
      "  monitor: val/loss\n",
      "  mode: min\n",
      "  filename: '{epoch:02d}-{val/loss:.4f}'\n",
      "  save_top_k: 3\n",
      "experiment:\n",
      "  name: default_experiment\n",
      "  description: Default autoencoder experiment\n",
      "\n",
      "Modelo: Autoencoder Clásico\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando entrenamiento en CUDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory E:\\IA\\TareaAutoEncoders\\checkpoints\\autoencoder_small_l1_comparison exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential | 93.2 K | train\n",
      "1 | flatten   | Flatten    | 0      | train\n",
      "2 | fc_mu     | Linear     | 4.2 M  | train\n",
      "3 | fc_decode | Linear     | 4.2 M  | train\n",
      "4 | decoder   | Sequential | 165 K  | train\n",
      "5 | criterion | L1Loss     | 0      | train\n",
      "-------------------------------------------------\n",
      "8.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.7 M     Total params\n",
      "34.721    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da19660490d4948845d38c796c5f76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Batch shape: torch.Size([32, 3, 128, 128]), necesito 4\n",
      " Rutas de validación (muestras que se usarán): ['DATASET_128x128\\\\train\\\\capsule_train_good_020.png', 'DATASET_128x128\\\\train\\\\transistor_train_good_034.png', 'DATASET_128x128\\\\train\\\\cable_train_good_159.png', 'DATASET_128x128\\\\train\\\\screw_train_good_084.png']\n",
      "Logueadas 4 imágenes. Total: 4\n",
      "Aplicando PCA a 146 muestras...\n",
      "Error en AnomalyDetectionVisualizer: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 2 has 1 dimension(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd25464cf6d4dc5b4c07ed6435eda43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c50e3e1cc24e5386f001682d87189c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Batch shape: torch.Size([32, 3, 128, 128]), necesito 4\n",
      " Rutas de validación (muestras que se usarán): ['DATASET_128x128\\\\train\\\\capsule_train_good_020.png', 'DATASET_128x128\\\\train\\\\transistor_train_good_034.png', 'DATASET_128x128\\\\train\\\\cable_train_good_159.png', 'DATASET_128x128\\\\train\\\\screw_train_good_084.png']\n",
      "Logueadas 4 imágenes. Total: 8\n",
      "Aplicando PCA a 146 muestras...\n",
      "Error en AnomalyDetectionVisualizer: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 2 has 1 dimension(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.138\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▅█</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_loss</td><td>0.14524</td></tr><tr><td>trainer/global_step</td><td>25</td></tr><tr><td>val_loss</td><td>0.13797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autoencoder_small_l1_comparison</strong> at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/n5q6mee6' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/n5q6mee6</a><br> View project at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251123_180747-n5q6mee6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 28] No space left on device\n",
      "\n",
      "[2/4] Entrenando autoencoder_small con L2...\n",
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria: 8.59 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\IA\\TareaAutoEncoders\\wandb\\run-20251123_180753-42pbtkvh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/42pbtkvh' target=\"_blank\">autoencoder_small_l2_comparison</a></strong> to <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/42pbtkvh' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/42pbtkvh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\hydra\\_internal\\defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ejecutando experimento: autoencoder_small_l2_comparison\n",
      "============================================================\n",
      "model:\n",
      "  name: autoencoder_small\n",
      "  in_channels: 3\n",
      "  hidden_dims:\n",
      "  - 32\n",
      "  - 64\n",
      "  - 128\n",
      "  latent_dim: 128\n",
      "  use_batch_norm: true\n",
      "  dropout_rate: 0.0\n",
      "trainer:\n",
      "  max_epochs: 20\n",
      "  gpus: 1\n",
      "  precision: 32\n",
      "  deterministic: true\n",
      "  check_val_every_n_epoch: 1\n",
      "  log_every_n_steps: 10\n",
      "  enable_model_summary: true\n",
      "  gradient_clip_val: 0.0\n",
      "  enable_progress_bar: true\n",
      "logger:\n",
      "  project: ae_experiments\n",
      "  entity: null\n",
      "  log_model: false\n",
      "  offline: false\n",
      "  tags: []\n",
      "loss:\n",
      "  type: MSELoss\n",
      "  name: L2\n",
      "optimizer:\n",
      "  name: adam_mid\n",
      "  type: Adam\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.0\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "seed: 42\n",
      "data:\n",
      "  data_dir: DATASET_128x128\n",
      "  image_size: 128\n",
      "  batch_size: 32\n",
      "  num_workers: 0\n",
      "  validation_split: 0.15\n",
      "  test_split: 0.15\n",
      "callbacks:\n",
      "  monitor: val/loss\n",
      "  mode: min\n",
      "  filename: '{epoch:02d}-{val/loss:.4f}'\n",
      "  save_top_k: 3\n",
      "experiment:\n",
      "  name: default_experiment\n",
      "  description: Default autoencoder experiment\n",
      "\n",
      "Modelo: Autoencoder Clásico\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando entrenamiento en CUDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory E:\\IA\\TareaAutoEncoders\\checkpoints\\autoencoder_small_l2_comparison exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential | 93.2 K | train\n",
      "1 | flatten   | Flatten    | 0      | train\n",
      "2 | fc_mu     | Linear     | 4.2 M  | train\n",
      "3 | fc_decode | Linear     | 4.2 M  | train\n",
      "4 | decoder   | Sequential | 165 K  | train\n",
      "5 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "8.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.7 M     Total params\n",
      "34.721    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9956d6944c4c2f854cea61b1f4bd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Batch shape: torch.Size([32, 3, 128, 128]), necesito 4\n",
      " Rutas de validación (muestras que se usarán): ['DATASET_128x128\\\\train\\\\capsule_train_good_020.png', 'DATASET_128x128\\\\train\\\\transistor_train_good_034.png', 'DATASET_128x128\\\\train\\\\cable_train_good_159.png', 'DATASET_128x128\\\\train\\\\screw_train_good_084.png']\n",
      "Logueadas 4 imágenes. Total: 4\n",
      "Aplicando PCA a 146 muestras...\n",
      "Error en AnomalyDetectionVisualizer: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 2 has 1 dimension(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Scripts\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625712cf1bfd49fbafd605c8687e9ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autoencoder_small_l2_comparison</strong> at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/42pbtkvh' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments/runs/42pbtkvh</a><br> View project at: <a href='https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments' target=\"_blank\">https://wandb.ai/fdbrenes17-tec-costa-rica/ae_experiments</a><br>Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251123_180753-42pbtkvh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Crear directorio para checkpoints\n",
    "    Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 INICIANDO EXPERIMENTACIÓN CON AUTOENCODERS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    # ② Entrenamientos con U-Net\n",
    "    print(\"\\nU-NET - Comparación de pérdidas\")\n",
    "    unet_results = run_all_experiments(model_name=\"unet\", num_loss_functions=4)\n",
    "    all_results.extend(unet_results)\n",
    "    \n",
    "    # ① Entrenamientos con Autoencoder\n",
    "    print(\"AUTOENCODER - Comparación de pérdidas\")\n",
    "    ae_results = run_all_experiments(model_name=\"autoencoder_small\", num_loss_functions=4)\n",
    "    all_results.extend(ae_results)\n",
    "    \n",
    "    # ③ Resumen final\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESUMEN FINAL DE EXPERIMENTACIÓN\")\n",
    "    print(\"=\"*70)\n",
    "    for result in all_results:\n",
    "        print(f\"  {result['model']:20} | {result['loss']:10} | {result['status']}\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72ca84",
   "metadata": {},
   "source": [
    "## Análisis cualitativo de las imágenes reconstruidas\n",
    "\n",
    "En esta sección comparamos visualmente las reconstrucciones obtenidas por:\n",
    "\n",
    "- El **autoencoder pequeño** (`autoencoder_small`)\n",
    "- La **U-Net** (`unet`)\n",
    "\n",
    "bajo cuatro funciones de pérdida diferentes:\n",
    "\n",
    "- **L1**\n",
    "- **L2**\n",
    "- **SSIM**\n",
    "- **SSIM + L1**\n",
    "\n",
    "La idea es observar qué tan bien cada modelo logra:\n",
    "\n",
    "1. Reconstruir correctamente las regiones **normales** de la imagen.\n",
    "2. Mantener o resaltar las **diferencias** en las regiones donde hay **defectos/anomalías** (zonas que queremos detectar).\n",
    "\n",
    "\n",
    "\n",
    "### Clase: cable\n",
    "\n",
    "#### Ejemplo de imagen cable con un error\n",
    "\n",
    "<img src=\"cable/test/bent_wire/002.png\" width=\"150\">\n",
    "\n",
    "#### Autoencoder pequeño\n",
    "\n",
    "![cable – Autoencoder pequeño (L1)](Images/cable/autoencoder_small_l1_comparison_cable.png)\n",
    "\n",
    "![cable – Autoencoder pequeño (L2)](Images/cable/autoencoder_small_l2_comparison_cable.png)\n",
    "\n",
    "![cable – Autoencoder pequeño (SSIM)](Images/cable/autoencoder_small_ssim_comparison_cable.png)\n",
    "\n",
    "![cable – Autoencoder pequeño (SSIM + L1)](Images/cable/autoencoder_small_ssim_l1_comparison_cable.png)\n",
    "\n",
    "#### U-Net\n",
    "\n",
    "![cable – U-Net (L1)](Images/cable/unet_l1_comparison_cable.png)\n",
    "\n",
    "![cable – U-Net (L2)](Images/cable/unet_l2_comparison_cable.png)\n",
    "\n",
    "![cable – U-Net (SSIM)](Images/cable/unet_ssim_comparison_cable.png)\n",
    "\n",
    "![cable – U-Net (SSIM + L1)](Images/cable/unet_ssim_l1_comparison_cable.png)\n",
    "\n",
    "**Analisis**\n",
    "\n",
    "- En general, la **U-Net** produce reconstrucciones más nítidas que el autoencoder pequeño, con mejor definición en bordes y estructura del cable.\n",
    "- El **autoencoder pequeño** tiende a generar reconstrucciones algo más borrosas, especialmente alrededor de las zonas donde hay defectos. Esto puede hacer que la diferencia entre imagen original y reconstruida sea menos clara.\n",
    "- Las pérdidas basadas en **SSIM** (SSIM y SSIM + L1) favorecen la preservación de la estructura global del cable, mientras que L1/L2 se enfocan más en errores pixel a pixel.\n",
    "- Visualmente, cuando hay defectos, las reconstrucciones que se ven “demasiado suaves” o “promediadas” tienden a ocultar parte de la anomalía, mientras que las reconstrucciones más fieles a las regiones sanas dejan más evidente el error en la zona defectuosa (mayor diferencia entre original y reconstruida).\n",
    "\n",
    "\n",
    "### Clase: capsule\n",
    "\n",
    "#### Ejemplo de una imagen de una cápsula con un error\n",
    "\n",
    "<img src=\"capsule/test/crack/001.png\" width=\"150\">\n",
    "\n",
    "#### Autoencoder pequeño\n",
    "\n",
    "![capsule – Autoencoder pequeño (L1)](Images/capsule/autoencoder_small_l1_comparison_capsule.png)\n",
    "\n",
    "![capsule – Autoencoder pequeño (L2)](Images/capsule/autoencoder_small_l2_comparison_capsule.png)\n",
    "\n",
    "![capsule – Autoencoder pequeño (SSIM)](Images/capsule/autoencoder_small_ssim_comparison_capsule.png)\n",
    "\n",
    "![capsule – Autoencoder pequeño (SSIM + L1)](Images/capsule/autoencoder_small_ssim_l1_comparison_capsule.png)\n",
    "\n",
    "#### U-Net\n",
    "\n",
    "![capsule – U-Net (L1)](Images/capsule/unet_l1_comparison_capsule.png)\n",
    "\n",
    "![capsule – U-Net (L2)](Images/capsule/unet_l2_comparison_capsule.png)\n",
    "\n",
    "![capsule – U-Net (SSIM)](Images/capsule/unet_ssim_comparison_capsule.png)\n",
    "\n",
    "![capsule – U-Net (SSIM + L1)](Images/capsule/unet_ssim_l1_comparison_capsule.png)\n",
    "\n",
    "**Analisis**\n",
    "\n",
    "- En las cápsulas es importante mantener la forma, contorno y textura de la superficie.\n",
    "- La **U-Net** suele conservar mejor las formas geométricas y los bordes, lo que facilita notar cuando una cápsula tiene daño o textura anómala.\n",
    "- El **autoencoder pequeño** puede introducir más suavizado, lo que dificulta separar visualmente una cápsula sana de una defectuosa.\n",
    "- Las variantes con **SSIM** tienden a respetar mejor el contraste y la estructura general de la cápsula, mientras que L1/L2 pueden producir reconstrucciones con menos ruido pero también menos detalle fino.\n",
    "\n",
    "\n",
    "### Clase: screw\n",
    "\n",
    "#### Ejemplo de una imagen de un tornillo con un error\n",
    "\n",
    "<img src=\"screw/test/scratch_head/008.png\" width=\"150\">\n",
    "\n",
    "#### Autoencoder pequeño\n",
    "\n",
    "![screw – Autoencoder pequeño (L1)](Images/screw/autoencoder_small_l1_comparison_screw.png)\n",
    "\n",
    "![screw – Autoencoder pequeño (L2)](Images/screw/autoencoder_small_l2_comparison_screw.png)\n",
    "\n",
    "![screw – Autoencoder pequeño (SSIM)](Images/screw/autoencoder_small_ssim_comparison_screw.png)\n",
    "\n",
    "![screw – Autoencoder pequeño (SSIM + L1)](Images/screw/autoencoder_small_ssim_l1_comparison_screw.png)\n",
    "\n",
    "#### U-Net\n",
    "\n",
    "![screw – U-Net (L1)](Images/screw/unet_l1_comparison_screw.png)\n",
    "\n",
    "![screw – U-Net (L2)](Images/screw/unet_l2_comparison_screw.png)\n",
    "\n",
    "![screw – U-Net (SSIM)](Images/screw/unet_ssim_comparison_screw.png)\n",
    "\n",
    "![screw – U-Net (SSIM + L1)](Images/screw/unet_ssim_l1_comparison_screw.png)\n",
    "\n",
    "**Analisis**\n",
    "\n",
    "- En tornillos, los detalles de la rosca y la geometría metálica son importantes.\n",
    "- La **U-Net** tiende a preservar mejor la forma de la rosca y los contornos metálicos, lo que ayuda a que los defectos resalten como diferencias claras entre original y reconstrucción.\n",
    "- El **autoencoder pequeño** puede perder detalles finos de textura, haciendo que las partes defectuosas se vean menos destacadas.\n",
    "- Otra observación típica es que las pérdidas **L2/SSIM** ayudan a reconstruir mejor el patrón repetitivo de la rosca, mientras que L1 puede producir bordes algo más “cortados” o menos suaves.\n",
    "\n",
    "\n",
    "### Clase: transistor\n",
    "\n",
    "#### Ejemplo de una imagen de un transistor con un error\n",
    "\n",
    "<img src=\"transistor/test/damaged_case/001.png\" width=\"150\">\n",
    "\n",
    "#### Autoencoder pequeño\n",
    "\n",
    "![transistor – Autoencoder pequeño (L1)](Images/transistor/autoencoder_small_l1_comparison_transistor.png)\n",
    "\n",
    "![transistor – Autoencoder pequeño (L2)](Images/transistor/autoencoder_small_l2_comparison_transistor.png)\n",
    "\n",
    "![transistor – Autoencoder pequeño (SSIM)](Images/transistor/autoencoder_small_ssim_comparison_transistor.png)\n",
    "\n",
    "![transistor – Autoencoder pequeño (SSIM + L1)](Images/transistor/autoencoder_small_ssim_l1_comparison_transistor.png)\n",
    "\n",
    "#### U-Net\n",
    "\n",
    "![transistor – U-Net (L1)](Images/transistor/unet_l1_comparison_transistor.png)\n",
    "\n",
    "![transistor – U-Net (L2)](Images/transistor/unet_l2_comparison_transistor.png)\n",
    "\n",
    "![transistor – U-Net (SSIM)](Images/transistor/unet_ssim_comparison_transistor.png)\n",
    "\n",
    "![transistor – U-Net (SSIM + L1)](Images/transistor/unet_ssim_l1_comparison_transistor.png)\n",
    "\n",
    "**Analisis**\n",
    "\n",
    "- En transistores hay muchos detalles pequeños: bordes de componentes, pistas, uniones, etc.\n",
    "- La **U-Net** suele manejar mejor estas estructuras locales complejas, reconstruyendo de forma más limpia las regiones sanas y dejando diferencias más claras donde hay anomalías.\n",
    "- El **autoencoder pequeño**, al tener menos capacidad, tiende a promediar más la información en regiones con muchos detalles, lo que puede “difuminar” parte de los defectos.\n",
    "- De nuevo, las pérdidas basadas en **SSIM** favorecen la coherencia estructural de la imagen, lo que es útil en este tipo de objetos con patrones finos.\n",
    "\n",
    "\n",
    "### Conclusiones generales del análisis cualitativo\n",
    "\n",
    "- En todas las clases (**cable, capsule, screw, transistor**) se observa un patrón similar:\n",
    "  - La **U-Net** produce reconstrucciones más detalladas y estructuralmente coherentes.\n",
    "  - El **autoencoder pequeño** genera imágenes más suaves y con menos detalle fino, especialmente en zonas complejas.\n",
    "- Respecto a las funciones de pérdida:\n",
    "  - Las pérdidas **L2** y **SSIM** suelen dar reconstrucciones muy limpias de las regiones sanas.\n",
    "  - Las pérdidas con **L1** tienden a ser un poco más robustas a outliers, pero pueden introducir artefactos o menos suavidad.\n",
    "  - Las combinaciones como **SSIM + L1** buscan un balance entre estructura global y penalización por error absoluto.\n",
    "- Para detección de anomalías, lo que interesa no es solo que la reconstrucción sea “bonita”, sino que:\n",
    "  1. Las partes **normales** se reconstruyan bien (error bajo).\n",
    "  2. Las partes **anómalas** se reconstruyan mal (error alto), de forma que el mapa de error permita separar claramente sano vs defectuoso.\n",
    "\n",
    "En la siguiente sección se puede complementar este análisis visual con métricas cuantitativas (por ejemplo, usando el `test_loss` de cada configuración) y discutir cómo se relaciona la calidad visual de las reconstrucciones con el desempeño numérico del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1916f",
   "metadata": {},
   "source": [
    "## Análisis cuantitativo de los resultados\n",
    "\n",
    "Esta sección presenta un análisis cuantitativo integral del desempeño de los modelos entrenados. Se incluyen:\n",
    "\n",
    "- Comparación de desempeño (test_loss)\n",
    "- La evolución de la pérdida durante el entrenamiento y validación.\n",
    "- Un análisis de convergencia para comparar estabilidad y velocidad de aprendizaje.\n",
    "- Interpretación del espacio latente para ambas arquitecturas.\n",
    "- Conclusiones sobre la capacidad de generalización y su relación con las funciones de pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9f6b8",
   "metadata": {},
   "source": [
    "## 1. Comparación de desempeño (test_loss)\n",
    "\n",
    "Los siguientes valores corresponden al error final en el conjunto de prueba para cada combinación de modelo y función de pérdida. Un valor más bajo indica mejor reconstrucción de las regiones normales y, por lo tanto, mejor capacidad de diferenciar anomalías.\n",
    "\n",
    "| Modelo                 | Función de pérdida | Test Loss     |\n",
    "|------------------------|--------------------|----------------|\n",
    "| unet                   | l2                 | 0.00054789     |\n",
    "| autoencoder_small      | l2                 | 0.0040282      |\n",
    "| unet                   | ssim               | 0.0051722      |\n",
    "| unet                   | ssim_l1            | 0.0064004      |\n",
    "| unet                   | l1                 | 0.019768       |\n",
    "| autoencoder_small      | l1                 | 0.036579       |\n",
    "| autoencoder_small      | ssim               | 0.16097        |\n",
    "| autoencoder_small      | ssim_l1            | 0.16594        |\n",
    "\n",
    "### Observaciones principales\n",
    "- El mejor resultado lo obtiene **U-Net con pérdida L2**, con un error extremadamente bajo.\n",
    "- El autoencoder pequeño obtiene su mejor resultado también con **L2**, pero se queda claramente atrás respecto a U-Net.\n",
    "- Las funciones basadas en **SSIM** obtienen valores más altos debido a que su métrica no está alineada con la medida usada en test_loss (que normalmente es MSE o similar).\n",
    "- El peor desempeño global corresponde a **autoencoder_small con SSIM y SSIM+L1**, lo cual coincide con las limitaciones visuales observadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9b0fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Curvas de entrenamiento y validación\n",
    "\n",
    "Las siguientes figuras muestran cómo evoluciona la pérdida durante el proceso de entrenamiento para todos los experimentos registrados. Incluyen promedios de las corridas cargadas y reflejan la estabilidad del proceso de optimización.\n",
    "\n",
    "### Pérdida de entrenamiento\n",
    "![train loss](wandb/lossFunction/train_loss.png)\n",
    "\n",
    "### Pérdida de validación\n",
    "![val loss](wandb/lossFunction/val_loss.png)\n",
    "\n",
    "### Interpretación de las curvas\n",
    "\n",
    "**1. Estabilidad del entrenamiento**\n",
    "- Las curvas de entrenamiento tienden a ser más suaves que las curvas de validación, lo cual es esperado.\n",
    "- U-Net en la mayoría de sus pérdidas converge de manera más estable que el autoencoder pequeño.\n",
    "- El autoencoder pequeño presenta fluctuaciones más marcadas, reflejando su limitada capacidad representacional.\n",
    "\n",
    "**2. Velocidad de convergencia**\n",
    "- La pérdida **L2** muestra la convergencia más rápida y estable en ambos modelos.\n",
    "- Las pérdidas basadas en SSIM convergen más lentamente y de manera menos monótona debido a que capturan estadísticas estructurales y no únicamente errores pixel a pixel.\n",
    "\n",
    "**3. Diferencia entre training y validation loss**\n",
    "- Para U-Net, la brecha entre ambas curvas es pequeña, lo que indica buena generalización.\n",
    "- En el autoencoder pequeño, la brecha es mayor, lo que sugiere sobreajuste por capacidad limitada.\n",
    "\n",
    "\n",
    "## 3. Interpretación del espacio latente\n",
    "\n",
    "Las siguientes imágenes muestran cómo cada modelo representa la información comprimida antes de la fase de decodificación.\n",
    "\n",
    "### Autoencoder – L1\n",
    "![ae_l1_latent](wandb/latentSpace/autoencoder_l1_comparison.png)\n",
    "\n",
    "### Autoencoder – L2\n",
    "![ae_l2_latent](wandb/latentSpace/autoencoder_l2_comparison.png)\n",
    "\n",
    "### Autoencoder – SSIM\n",
    "![ae_ssim_latent](wandb/latentSpace/autoencoder_ssim_comparison.png)\n",
    "\n",
    "### Autoencoder – SSIM + L1\n",
    "![ae_ssim_l1_latent](wandb/latentSpace/autoencoder_ssim_l1_comparison.png)\n",
    "\n",
    "### U-Net – L1\n",
    "![unet_l1_latent](wandb/latentSpace/unet_l1_comparison.png)\n",
    "\n",
    "### U-Net – L2\n",
    "![unet_l2_latent](wandb/latentSpace/unet_l2_comparison.png)\n",
    "\n",
    "### U-Net – SSIM\n",
    "![unet_ssim_latent](wandb/latentSpace/unet_ssim_comparison.png)\n",
    "\n",
    "### U-Net – SSIM + L1\n",
    "![unet_ssim_l1_latent](wandb/latentSpace/unet_ssim_l1_comparison.png)\n",
    "\n",
    "### Interpretación del espacio latente\n",
    "\n",
    "**1. Autoencoder**\n",
    "- El espacio latente es más difuso: representa la información de manera comprimida pero con pérdida notable.\n",
    "- Los vectores latentes muestran colapsos de información en configuraciones con SSIM, lo cual explica los altos `test_loss`.\n",
    "- Con L2, las representaciones son más compactas y separables, lo cual coincide con su mejor desempeño cuantitativo.\n",
    "\n",
    "**2. U-Net**\n",
    "- Aunque U-Net no tiene un “latente puro” como el autoencoder, sus activaciones internas muestran patrones mucho más estructurados.\n",
    "- Se observa mayor diferenciación entre regiones normales y anómalas incluso antes de la decodificación.\n",
    "- Todas las pérdidas producen representaciones más limpias que las del autoencoder pequeño.\n",
    "\n",
    "**3. Consistencia entre latente y reconstrucción**\n",
    "- Cuando el espacio latente está bien estructurado, la reconstrucción suele ser más precisa.\n",
    "- Esto se observa claramente en U-Net con L2 y SSIM.\n",
    "- Cuando el espacio latente es ruidoso (autoencoder + SSIM), la reconstrucción también se degrada.\n",
    "\n",
    "\n",
    "## 4. Relación entre test_loss y las curvas de entrenamiento\n",
    "\n",
    "El resumen comparativo de `test_loss`  muestra que:\n",
    "\n",
    "- U-Net con L2 alcanza el mejor error numérico debido a su capacidad para conservar detalles.\n",
    "- El autoencoder presenta mayor error tanto en entrenamiento como en prueba.\n",
    "- Las pérdidas basadas en SSIM no buscan minimizar el error pixel a pixel, lo que explica sus valores altos en `test_loss`.\n",
    "\n",
    "Al observar las gráficas:\n",
    "\n",
    "- Los modelos con menor test_loss corresponden también a curvas más suaves y monotónicas.\n",
    "- La pérdida de validación confirma que U-Net no solo aprende más rápido sino también más consistentemente.\n",
    "- El autoencoder muestra tendencia a caer en mínimos pobres, especialmente con funciones de pérdida no alineadas con el criterio de evaluación.\n",
    "\n",
    "\n",
    "## 5. Conclusiones cuantitativas\n",
    "\n",
    "1. **U-Net con pérdida L2 es la configuración más efectiva**, mostrando:\n",
    "   - La convergencia más estable.\n",
    "   - Las representaciones latentes más diferenciadas.\n",
    "   - El menor error final en validación y prueba.\n",
    "   - La mejor consistencia entre entrenamiento y validación.\n",
    "\n",
    "2. **Las funciones de pérdida basadas en SSIM generan resultados visualmente aceptables, pero no óptimos bajo la métrica de evaluación**, debido a que su enfoque es estructural, no pixel a pixel.\n",
    "\n",
    "3. **El autoencoder pequeño muestra limitaciones significativas**, especialmente cuando se utilizan pérdidas como SSIM y SSIM + L1, lo que se refleja tanto en las gráficas como en los valores finales.\n",
    "\n",
    "4. **La relación entre complejidad del modelo, función de pérdida y estabilidad de aprendizaje queda clara**:\n",
    "   - Modelos más capaces toleran pérdidas más complejas.\n",
    "   - Modelos más simples necesitan pérdidas estrictas como L2 para converger adecuadamente.\n",
    "\n",
    "Este análisis cuantitativo complementa el análisis visual previo y juntos permiten concluir cuál modelo y función de pérdida son más adecuados para la tarea de reconstrucción y detección de anomalías.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
