{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c802f3",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2ec7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfac406",
   "metadata": {},
   "source": [
    "### Se toman las 4 carpetas (cable, capsule, screw y transistor) y se separa su información de testing training y se juntan en un solo dataset, igualmente guardando las etiquetas y se setea el tamaño de cada imagen en 128x128 como se indica en el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62d525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "DATASETS = ['cable', 'capsule', 'screw', 'transistor']\n",
    "BASE_PATH = Path('../TareaAutoEncoders')\n",
    "OUTPUT_PATH = BASE_PATH / 'DATASET_128x128'\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "# Crear estructura de salida (carpetas planas)\n",
    "for split in ['train', 'test', 'ground_truth']:\n",
    "    (OUTPUT_PATH / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_and_save(src_path: Path, dest_dir: Path, prefix: str, is_mask=False):\n",
    "    \"\"\"Lee, redimensiona y guarda. Si is_mask usa INTER_NEAREST.\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(src_path), cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            print(f\"⚠️ No se pudo leer: {src_path}\")\n",
    "            return False\n",
    "        interp = cv2.INTER_NEAREST if is_mask else cv2.INTER_AREA\n",
    "        resized = cv2.resize(img, IMAGE_SIZE, interpolation=interp)\n",
    "        dest = dest_dir / f\"{prefix}_{src_path.stem}{src_path.suffix}\"\n",
    "        cv2.imwrite(str(dest), resized)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error con {src_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Procesar datasets\n",
    "for dataset in DATASETS:\n",
    "    base = BASE_PATH / dataset\n",
    "\n",
    "    # train -> normalmente sólo 'good' en estos datasets\n",
    "    train_dir = base / 'train'\n",
    "    if train_dir.exists():\n",
    "        for cls in train_dir.iterdir():\n",
    "            if not cls.is_dir(): \n",
    "                continue\n",
    "            for img in cls.glob('*.*'):\n",
    "                prefix = f\"{dataset}_train_{cls.name}\"\n",
    "                process_and_save(img, OUTPUT_PATH / 'train', prefix, is_mask=False)\n",
    "\n",
    "    # test -> incluir good y defectos\n",
    "    test_dir = base / 'test'\n",
    "    if test_dir.exists():\n",
    "        for cls in test_dir.iterdir():\n",
    "            if not cls.is_dir():\n",
    "                continue\n",
    "            for img in cls.glob('*.*'):\n",
    "                prefix = f\"{dataset}_test_{cls.name}\"\n",
    "                process_and_save(img, OUTPUT_PATH / 'test', prefix, is_mask=False)\n",
    "\n",
    "    # ground_truth -> máscaras (usar nearest)\n",
    "    gt_dir = base / 'ground_truth'\n",
    "    if gt_dir.exists():\n",
    "        for cls in gt_dir.iterdir():\n",
    "            if not cls.is_dir():\n",
    "                continue\n",
    "            for img in cls.glob('*.*'):\n",
    "                prefix = f\"{dataset}_gt_{cls.name}\"\n",
    "                process_and_save(img, OUTPUT_PATH / 'ground_truth', prefix, is_mask=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb3c98",
   "metadata": {},
   "source": [
    "## Configuración de los archivos Hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123bc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio conf/ creado\n"
     ]
    }
   ],
   "source": [
    "# Crear estructura base\n",
    "conf_path = Path(\"conf\")\n",
    "conf_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Directorio conf/ creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209cb794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Subdirectorios creados:\n",
      "   - conf/model/\n",
      "   - conf/trainer/\n",
      "   - conf/logger/\n",
      "   - conf/loss/\n",
      "   - conf/optimizer/\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Crear carpetas necesarias\n",
    "subdirs = [\"model\", \"trainer\", \"logger\", \"loss\", \"optimizer\"]\n",
    "for subdir in subdirs:\n",
    "    (conf_path / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✅ Subdirectorios creados:\")\n",
    "for subdir in subdirs:\n",
    "    print(f\"   - conf/{subdir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ae3b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variaciones de modelo creadas:\n",
      "   - autoencoder_latent_small (128)\n",
      "   - autoencoder_latent_large (1024)\n"
     ]
    }
   ],
   "source": [
    "# Celda: Crear variaciones de configuración para experimentos\n",
    "# Variación 1: Latent dim pequeño\n",
    "latent_small_yaml = \"\"\"name: autoencoder_latent_small\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256]\n",
    "latent_dim: 128\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_latent_small.yaml\", \"w\") as f:\n",
    "    f.write(latent_small_yaml)\n",
    "\n",
    "# Variación 2: Latent dim grande\n",
    "latent_large_yaml = \"\"\"name: autoencoder_latent_large\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256]\n",
    "latent_dim: 1024\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_latent_large.yaml\", \"w\") as f:\n",
    "    f.write(latent_large_yaml)\n",
    "\n",
    "print(\"Variaciones de modelo creadas:\")\n",
    "print(\"   - autoencoder_latent_small (128)\")\n",
    "print(\"   - autoencoder_latent_large (1024)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53282c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/config.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Crear conf/config.yaml (configuración principal)\n",
    "config_yaml = \"\"\"defaults:\n",
    "  - model: autoencoder\n",
    "  - trainer: default\n",
    "  - logger: wandb\n",
    "  - loss: l2\n",
    "  - optimizer: adam_mid\n",
    "\n",
    "seed: 42\n",
    "\n",
    "data:\n",
    "  data_dir: 'DATASET_128x128'\n",
    "  image_size: 128\n",
    "  batch_size: 32\n",
    "  num_workers: 2\n",
    "  validation_split: 0.15\n",
    "  test_split: 0.15\n",
    "\n",
    "callbacks:\n",
    "  monitor: \"val/loss\"\n",
    "  mode: \"min\"\n",
    "  filename: \"{epoch:02d}-{val/loss:.4f}\"\n",
    "  save_top_k: 3\n",
    "\n",
    "experiment:\n",
    "  name: \"default_experiment\"\n",
    "  description: \"Default autoencoder experiment\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/config.yaml\", \"w\") as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(\"conf/config.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e13d661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/autoencoder.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Crear modelos - conf/model/autoencoder.yaml\n",
    "autoencoder_yaml = \"\"\"name: autoencoder\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256]\n",
    "latent_dim: 512\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder.yaml\", \"w\") as f:\n",
    "    f.write(autoencoder_yaml)\n",
    "\n",
    "print(\"conf/model/autoencoder.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb5d4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/unet.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Crear modelos - conf/model/unet.yaml\n",
    "unet_yaml = \"\"\"name: unet\n",
    "in_channels: 3\n",
    "base_channels: 32\n",
    "depth: 4\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/unet.yaml\", \"w\") as f:\n",
    "    f.write(unet_yaml)\n",
    "\n",
    "print(\"conf/model/unet.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4fef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/autoencoder_small.yaml creado (latent_dim: 128)\n"
     ]
    }
   ],
   "source": [
    "# Celda 6: Variaciones de autoencoder con latent_dim pequeño\n",
    "autoencoder_small_yaml = \"\"\"name: autoencoder_small\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128]\n",
    "latent_dim: 128\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_small.yaml\", \"w\") as f:\n",
    "    f.write(autoencoder_small_yaml)\n",
    "\n",
    "print(\"conf/model/autoencoder_small.yaml creado (latent_dim: 128)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03c99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/autoencoder_large.yaml creado (latent_dim: 1024)\n"
     ]
    }
   ],
   "source": [
    "# Celda 7: Variaciones de autoencoder con latent_dim grande\n",
    "autoencoder_large_yaml = \"\"\"name: autoencoder_large\n",
    "in_channels: 3\n",
    "hidden_dims: [32, 64, 128, 256, 512]\n",
    "latent_dim: 1024\n",
    "use_batch_norm: true\n",
    "dropout_rate: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/autoencoder_large.yaml\", \"w\") as f:\n",
    "    f.write(autoencoder_large_yaml)\n",
    "\n",
    "print(\"conf/model/autoencoder_large.yaml creado (latent_dim: 1024)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3cdcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/l1.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 8: Funciones de pérdida - L1\n",
    "l1_yaml = \"\"\"name: l1\n",
    "type: L1Loss\n",
    "weight: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/l1.yaml\", \"w\") as f:\n",
    "    f.write(l1_yaml)\n",
    "\n",
    "print(\"conf/loss/l1.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae19c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/l2.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 9: Funciones de pérdida - L2 (MSE)\n",
    "l2_yaml = \"\"\"name: l2\n",
    "type: MSELoss\n",
    "weight: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/l2.yaml\", \"w\") as f:\n",
    "    f.write(l2_yaml)\n",
    "\n",
    "print(\"conf/loss/l2.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818cd767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/ssim.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 10: Funciones de pérdida - SSIM\n",
    "ssim_yaml = \"\"\"name: ssim\n",
    "type: SSIMLoss\n",
    "weight: 1.0\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/ssim.yaml\", \"w\") as f:\n",
    "    f.write(ssim_yaml)\n",
    "\n",
    "print(\"conf/loss/ssim.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e24c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/loss/ssim_l1.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 11: Funciones de pérdida - SSIM + L1\n",
    "ssim_l1_yaml = \"\"\"name: ssim_l1\n",
    "type: SSIMLoss_L1\n",
    "weight_ssim: 0.5\n",
    "weight_l1: 0.5\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/ssim_l1.yaml\", \"w\") as f:\n",
    "    f.write(ssim_l1_yaml)\n",
    "\n",
    "print(\"conf/loss/ssim_l1.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f27b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/trainer/default.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 12: Trainer - conf/trainer/default.yaml\n",
    "trainer_yaml = \"\"\"max_epochs: 20\n",
    "gpus: 1\n",
    "precision: 32\n",
    "deterministic: true\n",
    "check_val_every_n_epoch: 1\n",
    "log_every_n_steps: 10\n",
    "enable_model_summary: true\n",
    "gradient_clip_val: 0.0\n",
    "enable_progress_bar: true\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/trainer/default.yaml\", \"w\") as f:\n",
    "    f.write(trainer_yaml)\n",
    "\n",
    "print(\"conf/trainer/default.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ea0d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/logger/wandb.yaml creado\n"
     ]
    }
   ],
   "source": [
    "# Celda 13: Logger - conf/logger/wandb.yaml\n",
    "wandb_yaml = \"\"\"project: ae_experiments\n",
    "entity: null\n",
    "log_model: false\n",
    "offline: false\n",
    "tags: []\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/logger/wandb.yaml\", \"w\") as f:\n",
    "    f.write(wandb_yaml)\n",
    "\n",
    "print(\"conf/logger/wandb.yaml creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01369ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/optimizer/adam_low.yaml creado (lr: 1e-4)\n"
     ]
    }
   ],
   "source": [
    "# Celda 14: Optimizer - Adam con LR bajo\n",
    "adam_low_yaml = \"\"\"name: adam_low\n",
    "type: Adam\n",
    "lr: 1e-4\n",
    "weight_decay: 0.0\n",
    "betas: [0.9, 0.999]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/optimizer/adam_low.yaml\", \"w\") as f:\n",
    "    f.write(adam_low_yaml)\n",
    "\n",
    "print(\"conf/optimizer/adam_low.yaml creado (lr: 1e-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eafba72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/optimizer/adam_mid.yaml creado (lr: 1e-3)\n"
     ]
    }
   ],
   "source": [
    "# Celda 15: Optimizer - Adam con LR medio\n",
    "adam_mid_yaml = \"\"\"name: adam_mid\n",
    "type: Adam\n",
    "lr: 1e-3\n",
    "weight_decay: 0.0\n",
    "betas: [0.9, 0.999]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/optimizer/adam_mid.yaml\", \"w\") as f:\n",
    "    f.write(adam_mid_yaml)\n",
    "\n",
    "print(\"conf/optimizer/adam_mid.yaml creado (lr: 1e-3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f83b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/optimizer/adam_high.yaml creado (lr: 5e-3)\n"
     ]
    }
   ],
   "source": [
    "# Celda 16: Optimizer - Adam con LR alto\n",
    "adam_high_yaml = \"\"\"name: adam_high\n",
    "type: Adam\n",
    "lr: 5e-3\n",
    "weight_decay: 1e-5\n",
    "betas: [0.9, 0.999]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/optimizer/adam_high.yaml\", \"w\") as f:\n",
    "    f.write(adam_high_yaml)\n",
    "\n",
    "print(\"conf/optimizer/adam_high.yaml creado (lr: 5e-3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8291b",
   "metadata": {},
   "source": [
    "## Definición del DataModule y modelo base (Autoencoder clásico con Hydra + PyTorch Lightning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff540c",
   "metadata": {},
   "source": [
    "### Definición del Dataset para DATASET_128x128\n",
    "\n",
    "En esta sección definimos una clase `MVTecDataset` basada en `torch.utils.data.Dataset`\n",
    "que carga las imágenes ya preprocesadas a tamaño **128×128**.\n",
    "\n",
    "Las imágenes se cargan en formato RGB y se convierten a tensores normalizados en \\[0, 1].\n",
    "Este dataset se utilizará dentro del `LightningDataModule` para separar train/val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90fd13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3402efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
    "        super().__init__()\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        split_dir = self.root_dir / split\n",
    "        exts = (\".png\", \".jpg\", \".jpeg\")\n",
    "        self.image_paths = [\n",
    "            p for p in split_dir.glob(\"*.*\") if p.suffix.lower() in exts\n",
    "        ]\n",
    "\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(f\"[WARNING] No se encontraron imágenes en {split_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Para autoencoder solo necesitamos la imagen (entrada = salida)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235779d1",
   "metadata": {},
   "source": [
    "### LightningDataModule para MVTec\n",
    "\n",
    "Para estructurar el flujo de datos usando PyTorch Lightning, se define un\n",
    "`LightningDataModule` llamado `MVTecDataModule`.\n",
    "\n",
    "Este módulo:\n",
    "\n",
    "- Recibe los hiperparámetros desde la configuración (`cfg.data`):\n",
    "  - `data_dir`, `batch_size`, `num_workers`, `validation_split`.\n",
    "- Construye el `Dataset` de entrenamiento completo y lo separa en:\n",
    "  - subconjunto de **train**\n",
    "  - subconjunto de **validation** (usando `validation_split`).\n",
    "- Crea el `Dataset` de **test**.\n",
    "- Expone los `DataLoader`:\n",
    "  - `train_dataloader()`\n",
    "  - `val_dataloader()`\n",
    "  - `test_dataloader()`\n",
    "\n",
    "De esta forma, el mismo `DataModule` se reutiliza para todos los modelos y\n",
    "experimentos (distintas funciones de pérdida, arquitecturas, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a39e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        val_split=0.15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "\n",
    "        # Transformación básica: convertir a tensor en [0,1]\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Dataset completo de entrenamiento (después se divide en train/val)\n",
    "        full_train = MVTecDataset(\n",
    "            root_dir=self.data_dir,\n",
    "            split=\"train\",\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "        n_total = len(full_train)\n",
    "        n_val = int(self.val_split * n_total)\n",
    "        n_train = n_total - n_val\n",
    "\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(\n",
    "            full_train,\n",
    "            [n_train, n_val],\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "\n",
    "        # Dataset de test\n",
    "        self.test_set = MVTecDataset(\n",
    "            root_dir=self.data_dir,\n",
    "            split=\"test\",\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdfeab",
   "metadata": {},
   "source": [
    "### Construcción de función de pérdida y optimizador desde Hydra\n",
    "\n",
    "La configuración de la tarea se maneja con Hydra. En particular:\n",
    "\n",
    "- `conf/loss/*.yaml` define el tipo de función de pérdida a usar\n",
    "  (L1, L2, SSIM, SSIM + L1, etc.).\n",
    "- `conf/optimizer/*.yaml` define el tipo de optimizador y sus hiperparámetros\n",
    "  (por ejemplo, Adam con distintas tasas de aprendizaje).\n",
    "\n",
    "Para desacoplar el modelo de estas decisiones, se implementan dos funciones:\n",
    "\n",
    "- `build_loss(loss_cfg)`: a partir de `cfg.loss` devuelve un objeto de pérdida\n",
    "  de PyTorch.\n",
    "- `build_optimizer(optimizer_cfg, parameters)`: a partir de `cfg.optimizer`\n",
    "  devuelve una instancia del optimizador apropiado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a64429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(loss_cfg):\n",
    "    \"\"\"\n",
    "    Construye la función de pérdida a partir de cfg.loss.\n",
    "    Por ahora se implementan L1 y L2 (MSE).\n",
    "    \"\"\"\n",
    "    loss_type = loss_cfg.type\n",
    "\n",
    "    if loss_type == \"L1Loss\":\n",
    "        return nn.L1Loss()\n",
    "    elif loss_type == \"MSELoss\":\n",
    "        return nn.MSELoss()\n",
    "    else:\n",
    "        # Aquí luego se agregarán SSIM y SSIM+L1\n",
    "        raise NotImplementedError(f\"Pérdida '{loss_type}' aún no implementada en este notebook.\")\n",
    "    \n",
    "\n",
    "def build_optimizer(optimizer_cfg, parameters):\n",
    "    \"\"\"\n",
    "    Construye el optimizador a partir de cfg.optimizer.\n",
    "    Actualmente soporta Adam con lr configurable.\n",
    "    \"\"\"\n",
    "    opt_type = optimizer_cfg.type\n",
    "\n",
    "    if opt_type == \"Adam\":\n",
    "        return torch.optim.Adam(\n",
    "            parameters,\n",
    "            lr=optimizer_cfg.lr,\n",
    "            weight_decay=optimizer_cfg.weight_decay,\n",
    "            betas=tuple(optimizer_cfg.betas),\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer '{opt_type}' no implementado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84d0f0",
   "metadata": {},
   "source": [
    "### Modelo base: Autoencoder clásico con PyTorch Lightning\n",
    "\n",
    "En esta sección se define el modelo de **autoencoder clásico** como un\n",
    "`LightningModule` llamado `LitAutoencoder`.\n",
    "\n",
    "Este módulo:\n",
    "\n",
    "- Lee su configuración desde `cfg.model`:\n",
    "  - `in_channels`\n",
    "  - `hidden_dims` (lista de canales intermedios)\n",
    "  - `latent_dim`\n",
    "- Construye un **encoder** convolucional que reduce la resolución de la imagen.\n",
    "- Aplica capas totalmente conectadas para:\n",
    "  - Proyectar la salida del encoder a un espacio latente de dimensión `latent_dim`.\n",
    "  - Reconstruir desde el espacio latente a la forma intermedia del decoder.\n",
    "- Construye un **decoder** con convoluciones transpuestas para recuperar\n",
    "  una imagen de tamaño 128×128 y 3 canales.\n",
    "- Utiliza la función de pérdida definida en `cfg.loss`.\n",
    "- Utiliza el optimizador definido en `cfg.optimizer`.\n",
    "\n",
    "Este modelo será el primero en usarse para los experimentos de la tarea\n",
    "(con distintas funciones de pérdida). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "805bbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, model_cfg, loss_cfg, optimizer_cfg, image_size=128):\n",
    "        super().__init__()\n",
    "        # Guardamos la config de modelo como hyperparams (para reproducibilidad)\n",
    "        self.save_hyperparameters(OmegaConf.to_container(model_cfg, resolve=True))\n",
    "\n",
    "        self.model_cfg = model_cfg\n",
    "        self.loss_cfg = loss_cfg\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.image_size = image_size\n",
    "\n",
    "        in_channels = model_cfg.in_channels\n",
    "        hidden_dims = list(model_cfg.hidden_dims)\n",
    "        latent_dim = model_cfg.latent_dim\n",
    "\n",
    "        # Encoder: secuencia de convoluciones con stride 2\n",
    "        modules = []\n",
    "        channels = in_channels\n",
    "        size = image_size\n",
    "        for h in hidden_dims:\n",
    "            modules.append(nn.Conv2d(channels, h, kernel_size=3, stride=2, padding=1))\n",
    "            modules.append(nn.ReLU())\n",
    "            channels = h\n",
    "            size = size // 2  # cada conv con stride 2 reduce la mitad\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.enc_out_channels = channels\n",
    "        self.enc_out_size = size\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(channels * size * size, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, channels * size * size)\n",
    "\n",
    "        # Decoder: conv transpuestas para volver a 3x128x128\n",
    "        modules = []\n",
    "        hidden_dims_rev = list(hidden_dims[::-1])\n",
    "\n",
    "        for i in range(len(hidden_dims_rev) - 1):\n",
    "            modules.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    hidden_dims_rev[i],\n",
    "                    hidden_dims_rev[i + 1],\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            modules.append(nn.ReLU())\n",
    "\n",
    "        modules.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims_rev[-1],\n",
    "                in_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            )\n",
    "        )\n",
    "        modules.append(nn.Sigmoid())  # salida en [0,1]\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        # Pérdida\n",
    "        self.criterion = build_loss(loss_cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        x_flat = self.flatten(x_enc)\n",
    "        z = self.fc_mu(x_flat)\n",
    "        x_dec_flat = self.fc_decode(z)\n",
    "        x_dec = x_dec_flat.view(\n",
    "            x.shape[0],\n",
    "            self.enc_out_channels,\n",
    "            self.enc_out_size,\n",
    "            self.enc_out_size,\n",
    "        )\n",
    "        x_hat = self.decoder(x_dec)\n",
    "        return x_hat\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = self.criterion(x_hat, x)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = build_optimizer(self.optimizer_cfg, self.parameters())\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a8978",
   "metadata": {},
   "source": [
    "### Función principal de entrenamiento con Hydra y WandB\n",
    "\n",
    "Finalmente, se define una función `train_autoencoder_with_hydra()` que\n",
    "integra todos los componentes anteriores:\n",
    "\n",
    "1. Inicializa Hydra y carga la configuración desde `conf/config.yaml`.\n",
    "2. Construye el `MVTecDataModule` usando `cfg.data`.\n",
    "3. Construye el `LitAutoencoder` usando:\n",
    "   - `cfg.model` (arquitectura del autoencoder clásico),\n",
    "   - `cfg.loss` (función de pérdida),\n",
    "   - `cfg.optimizer` (optimizador).\n",
    "4. Inicializa un `WandbLogger` con los parámetros de `cfg.logger`.\n",
    "5. Crea un `Trainer` de PyTorch Lightning con los parámetros definidos en `cfg.trainer`.\n",
    "6. Llama a `trainer.fit(model, datamodule=dm)` para entrenar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce1d2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_with_hydra():\n",
    "    \"\"\"\n",
    "    Función de entrenamiento principal.\n",
    "    Usa Hydra para cargar conf/config.yaml y los subarchivos.\n",
    "    \"\"\"\n",
    "    with initialize(config_path=\"conf\", version_base=None):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "\n",
    "    print(\"Configuración cargada:\")\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # DataModule\n",
    "    dm = MVTecDataModule(\n",
    "        data_dir=cfg.data.data_dir,\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        val_split=cfg.data.validation_split,\n",
    "    )\n",
    "\n",
    "    # Modelo (autoencoder clásico)\n",
    "    model = LitAutoencoder(\n",
    "        model_cfg=cfg.model,\n",
    "        loss_cfg=cfg.loss,\n",
    "        optimizer_cfg=cfg.optimizer,\n",
    "        image_size=cfg.data.image_size,\n",
    "    )\n",
    "\n",
    "    # Logger de WandB\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=cfg.logger.project,\n",
    "        entity=cfg.logger.entity,\n",
    "        log_model=cfg.logger.log_model,\n",
    "    )\n",
    "\n",
    "    # Trainer a partir de cfg.trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.trainer.max_epochs,\n",
    "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "        deterministic=cfg.trainer.deterministic,\n",
    "        enable_model_summary=cfg.trainer.enable_model_summary,\n",
    "        enable_progress_bar=cfg.trainer.enable_progress_bar,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c11f34",
   "metadata": {},
   "source": [
    "### Configuraciones de funciones de pérdida (Hydra)\n",
    "\n",
    "Se requiere evaluar distintas funciones de pérdida para el autoencoder:\n",
    "\n",
    "- **L1**  \n",
    "- **L2 (MSE)**  \n",
    "- **SSIM**  \n",
    "- **SSIM + L1**\n",
    "\n",
    "Para permitir cambiar entre estas variantes desde Hydra sin modificar código,\n",
    "se definen cuatro archivos de configuración en `conf/loss/`:\n",
    "\n",
    "- `l1.yaml`\n",
    "- `l2.yaml`\n",
    "- `ssim.yaml`\n",
    "- `ssim_l1.yaml`\n",
    "\n",
    "Cada uno especifica el tipo de pérdida a usar (`type`) y, en el caso de SSIM,\n",
    "algunos hiperparámetros adicionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8083a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos de configuración de pérdidas creados/actualizados en conf/loss/\n"
     ]
    }
   ],
   "source": [
    "Path(\"conf/loss\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "l1_yaml = \"\"\"type: L1Loss\n",
    "name: \"L1\"\n",
    "\"\"\"\n",
    "\n",
    "l2_yaml = \"\"\"type: MSELoss\n",
    "name: \"L2\"\n",
    "\"\"\"\n",
    "\n",
    "ssim_yaml = \"\"\"type: SSIM\n",
    "name: \"SSIM\"\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "\"\"\"\n",
    "\n",
    "ssim_l1_yaml = \"\"\"type: SSIM_L1\n",
    "name: \"SSIM+L1\"\n",
    "window_size: 11\n",
    "sigma: 1.5\n",
    "data_range: 1.0\n",
    "l1_weight: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/loss/l1.yaml\", \"w\") as f:\n",
    "    f.write(l1_yaml)\n",
    "\n",
    "with open(\"conf/loss/l2.yaml\", \"w\") as f:\n",
    "    f.write(l2_yaml)\n",
    "\n",
    "with open(\"conf/loss/ssim.yaml\", \"w\") as f:\n",
    "    f.write(ssim_yaml)\n",
    "\n",
    "with open(\"conf/loss/ssim_l1.yaml\", \"w\") as f:\n",
    "    f.write(ssim_l1_yaml)\n",
    "\n",
    "print(\"Archivos de configuración de pérdidas creados/actualizados en conf/loss/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70f880",
   "metadata": {},
   "source": [
    "### Implementación de la pérdida SSIM\n",
    "\n",
    "La métrica **Structural Similarity Index (SSIM)** mide la similitud estructural\n",
    "entre dos imágenes. A diferencia de L1/L2, que comparan píxel a píxel,\n",
    "SSIM toma en cuenta:\n",
    "\n",
    "- luminancia,\n",
    "- contraste,\n",
    "- estructura local.\n",
    "\n",
    "Para usar SSIM como pérdida, se suele minimizar `1 - SSIM(x, y)`, donde `x` es\n",
    "la imagen original y `y` la reconstrucción del autoencoder.\n",
    "\n",
    "A continuación se define una implementación en PyTorch que:\n",
    "\n",
    "- Convierte la fórmula de SSIM a operaciones de convolución 2D con un kernel\n",
    "  gaussiano.\n",
    "- Calcula SSIM de forma local y luego promedia el resultado.\n",
    "- Devuelve `1 - SSIM` como valor de pérdida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e81108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size=11,\n",
    "        sigma=1.5,\n",
    "        data_range=1.0,\n",
    "        channel=3,\n",
    "        K1=0.01,\n",
    "        K2=0.03,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementación de SSIM como pérdida: loss = 1 - SSIM.\n",
    "        Asume imágenes en rango [0, data_range] y 3 canales por defecto.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.sigma = sigma\n",
    "        self.data_range = data_range\n",
    "        self.channel = channel\n",
    "        self.K1 = K1\n",
    "        self.K2 = K2\n",
    "\n",
    "        self.register_buffer(\"window\", self._create_window(window_size, sigma, channel))\n",
    "\n",
    "    def _gaussian(self, window_size, sigma):\n",
    "        gauss = torch.tensor(\n",
    "            [\n",
    "                (-(x - window_size // 2) ** 2) / float(2 * sigma**2)\n",
    "                for x in range(window_size)\n",
    "            ]\n",
    "        )\n",
    "        gauss = torch.exp(gauss)\n",
    "        return gauss / gauss.sum()\n",
    "\n",
    "    def _create_window(self, window_size, sigma, channel):\n",
    "        _1d_window = self._gaussian(window_size, sigma).unsqueeze(1)\n",
    "        _2d_window = _1d_window @ _1d_window.t()  # producto externo\n",
    "        _2d_window = _2d_window.float().unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "        window = _2d_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        x, y: tensores [B, C, H, W] en rango [0, data_range]\n",
    "        Devuelve 1 - SSIM promedio en el batch.\n",
    "        \"\"\"\n",
    "        if x.size(1) != self.channel or y.size(1) != self.channel:\n",
    "            # Simplemente adaptamos el canal si es distinto (por si acaso)\n",
    "            self.channel = x.size(1)\n",
    "            self.window = self._create_window(self.window_size, self.sigma, self.channel).to(x.device)\n",
    "\n",
    "        # Constantes de SSIM\n",
    "        C1 = (self.K1 * self.data_range) ** 2\n",
    "        C2 = (self.K2 * self.data_range) ** 2\n",
    "\n",
    "        # Media local\n",
    "        mu_x = torch.nn.functional.conv2d(\n",
    "            x, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        )\n",
    "        mu_y = torch.nn.functional.conv2d(\n",
    "            y, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        )\n",
    "\n",
    "        mu_x2 = mu_x * mu_x\n",
    "        mu_y2 = mu_y * mu_y\n",
    "        mu_xy = mu_x * mu_y\n",
    "\n",
    "        # Varianzas y covarianza\n",
    "        sigma_x2 = torch.nn.functional.conv2d(\n",
    "            x * x, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        ) - mu_x2\n",
    "        sigma_y2 = torch.nn.functional.conv2d(\n",
    "            y * y, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        ) - mu_y2\n",
    "        sigma_xy = torch.nn.functional.conv2d(\n",
    "            x * y, self.window, padding=self.window_size // 2, groups=self.channel\n",
    "        ) - mu_xy\n",
    "\n",
    "        # Fórmula de SSIM\n",
    "        num = (2 * mu_xy + C1) * (2 * sigma_xy + C2)\n",
    "        den = (mu_x2 + mu_y2 + C1) * (sigma_x2 + sigma_y2 + C2)\n",
    "\n",
    "        ssim_map = num / (den + 1e-8)\n",
    "        ssim = ssim_map.mean()\n",
    "\n",
    "        # Pérdida = 1 - SSIM promedio\n",
    "        loss = 1 - ssim\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e06b0a",
   "metadata": {},
   "source": [
    "### Actualización de `build_loss` para incluir SSIM y SSIM+L1\n",
    "\n",
    "Con la clase `SSIMLoss` definida, se extiende la función `build_loss` para\n",
    "reconocer cuatro tipos de pérdida:\n",
    "\n",
    "- `L1Loss`  → L1 estándar.\n",
    "- `MSELoss` → L2 (MSE).\n",
    "- `SSIM`    → `1 - SSIM(x, y)`.\n",
    "- `SSIM_L1` → combinación lineal de SSIM y L1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "259180f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(loss_cfg):\n",
    "    \"\"\"\n",
    "    Construye la función de pérdida a partir de cfg.loss.\n",
    "\n",
    "    Soporta:\n",
    "      - L1Loss\n",
    "      - MSELoss\n",
    "      - SSIM\n",
    "      - SSIM_L1 (combinación SSIM + L1)\n",
    "    \"\"\"\n",
    "    loss_type = loss_cfg.type\n",
    "\n",
    "    if loss_type == \"L1Loss\":\n",
    "        return nn.L1Loss()\n",
    "\n",
    "    elif loss_type == \"MSELoss\":\n",
    "        return nn.MSELoss()\n",
    "\n",
    "    elif loss_type == \"SSIM\":\n",
    "        return SSIMLoss(\n",
    "            window_size=loss_cfg.window_size,\n",
    "            sigma=loss_cfg.sigma,\n",
    "            data_range=loss_cfg.data_range,\n",
    "            channel=3,  # nuestras imágenes son RGB\n",
    "        )\n",
    "\n",
    "    elif loss_type == \"SSIM_L1\":\n",
    "        ssim_loss = SSIMLoss(\n",
    "            window_size=loss_cfg.window_size,\n",
    "            sigma=loss_cfg.sigma,\n",
    "            data_range=loss_cfg.data_range,\n",
    "            channel=3,\n",
    "        )\n",
    "        l1 = nn.L1Loss()\n",
    "        l1_weight = loss_cfg.l1_weight\n",
    "\n",
    "        class SSIML1Combined(nn.Module):\n",
    "            def __init__(self, ssim_loss, l1, l1_weight):\n",
    "                super().__init__()\n",
    "                self.ssim_loss = ssim_loss\n",
    "                self.l1 = l1\n",
    "                self.l1_weight = l1_weight\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                loss_ssim = self.ssim_loss(x, y)        # 1 - SSIM\n",
    "                loss_l1 = self.l1(x, y)\n",
    "                return loss_ssim + self.l1_weight * loss_l1\n",
    "\n",
    "        return SSIML1Combined(ssim_loss, l1, l1_weight)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Pérdida '{loss_type}' aún no implementada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d536a7d",
   "metadata": {},
   "source": [
    "### Configuración del modelo U-Net (Hydra)\n",
    "\n",
    "Se requiere evaluar un autoencoder clásico y un autoencoder tipo **U-Net**.\n",
    "\n",
    "Para permitir seleccionar esta arquitectura desde Hydra sin modificar el código,\n",
    "se crea el archivo `conf/model/unet.yaml`, donde se definen sus parámetros:\n",
    "\n",
    "- `in_channels`: número de canales de entrada (3 para RGB)\n",
    "- `base_channels`: número inicial de filtros del encoder\n",
    "- `depth`: cantidad de niveles de downsampling / upsampling\n",
    "- `latent_dim`: tamaño del cuello (opcional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f99ca031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf/model/unet.yaml creado\n"
     ]
    }
   ],
   "source": [
    "Path(\"conf/model\").mkdir(exist_ok=True)\n",
    "\n",
    "unet_yaml = \"\"\"in_channels: 3\n",
    "base_channels: 32\n",
    "depth: 4\n",
    "latent_dim: 128\n",
    "\"\"\"\n",
    "\n",
    "with open(\"conf/model/unet.yaml\", \"w\") as f:\n",
    "    f.write(unet_yaml)\n",
    "\n",
    "print(\"conf/model/unet.yaml creado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd082bc",
   "metadata": {},
   "source": [
    "### Implementación del Autoencoder U-Net\n",
    "\n",
    "Este modelo sigue una estructura típica de U-Net:\n",
    "\n",
    "1. **Encoder**:\n",
    "   - Múltiples niveles de convoluciones + downsampling (stride 2).\n",
    "   - Se almacenan características para las conexiones tipo \"skip\".\n",
    "\n",
    "2. **Bottleneck**:\n",
    "   - Capa completamente conectada para pasar al espacio latente.\n",
    "\n",
    "3. **Decoder**:\n",
    "   - ConvTransposed2D para upsampling simétrico.\n",
    "   - Se concatenan los \"skip connections\" del encoder.\n",
    "\n",
    "Este modelo debe:\n",
    "- Usar la misma función de pérdida configurada en `cfg.loss`\n",
    "- Usar el mismo optimizador configurado en `cfg.optimizer`\n",
    "- Ser llamado desde Hydra con:\n",
    "  `defaults: - model: unet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73f64b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNetDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(out_ch * 2, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class LitUNetAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, model_cfg, loss_cfg, optimizer_cfg):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(OmegaConf.to_container(model_cfg, resolve=True))\n",
    "\n",
    "        self.model_cfg = model_cfg\n",
    "        self.loss_cfg = loss_cfg\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "\n",
    "        base = model_cfg.base_channels\n",
    "        depth = model_cfg.depth\n",
    "        in_ch = model_cfg.in_channels\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        self.enc_blocks = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        ch = in_ch\n",
    "        channels = []\n",
    "\n",
    "        for d in range(depth):\n",
    "            out_ch = base * (2 ** d)\n",
    "            self.enc_blocks.append(UNetEncoderBlock(ch, out_ch))\n",
    "            channels.append(out_ch)\n",
    "            ch = out_ch\n",
    "            self.downsamples.append(nn.Conv2d(out_ch, out_ch, kernel_size=2, stride=2))\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetEncoderBlock(ch, ch * 2)\n",
    "\n",
    "        # ----- Decoder -----\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        ch = ch * 2\n",
    "\n",
    "        for d in reversed(range(depth)):\n",
    "            out_ch = base * (2 ** d)\n",
    "            self.up_blocks.append(nn.ConvTranspose2d(ch, out_ch, kernel_size=2, stride=2))\n",
    "            self.dec_blocks.append(UNetDecoderBlock(out_ch, out_ch))\n",
    "            ch = out_ch\n",
    "\n",
    "        # Output (3 canales)\n",
    "        self.final_conv = nn.Conv2d(base, 3, kernel_size=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = build_loss(loss_cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        out = x\n",
    "\n",
    "        # Encoder\n",
    "        for enc, down in zip(self.enc_blocks, self.downsamples):\n",
    "            out = enc(out)\n",
    "            skips.append(out)\n",
    "            out = down(out)\n",
    "\n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        # Decoder\n",
    "        for up, dec, skip in zip(self.up_blocks, self.dec_blocks, reversed(skips)):\n",
    "            out = up(out)\n",
    "            out = torch.cat([out, skip], dim=1)\n",
    "            out = dec(out, skip)\n",
    "\n",
    "        out = self.final_conv(out)\n",
    "        return self.activation(out)\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = self.criterion(x_hat, x)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return build_optimizer(self.optimizer_cfg, self.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2f92b",
   "metadata": {},
   "source": [
    "### Función de entrenamiento para soportar Autoencoder y U-Net\n",
    "\n",
    "Se reescribe la función `train_autoencoder_with_hydra()` para:\n",
    "\n",
    "- Cargar la configuración completa desde Hydra.\n",
    "- Crear automáticamente el DataModule.\n",
    "- Instanciar el modelo según `cfg.model`:\n",
    "  - `autoencoder` → `LitAutoencoder`\n",
    "  - `unet` → `LitUNetAutoencoder`\n",
    "- Inicializar WandB.\n",
    "- Crear un Trainer de Lightning.\n",
    "- Entrenar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eeb15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_with_hydra():\n",
    "    \"\"\"\n",
    "    Versión actualizada: soporta modelos 'autoencoder' y 'unet'.\n",
    "    Utiliza Hydra + Lightning + WandB para ejecutar entrenamientos reproducibles.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Cargar configuración completa desde Hydra\n",
    "    with initialize(config_path=\"conf\", version_base=None):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "\n",
    "    print(\"=========== CONFIGURACIÓN CARGADA ===========\")\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    # 2. Crear DataModule con los parámetros de cfg.data\n",
    "    dm = MVTecDataModule(\n",
    "        data_dir=cfg.data.data_dir,\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        val_split=cfg.data.validation_split,\n",
    "    )\n",
    "\n",
    "    # 3. Instanciar modelo según cfg.model\n",
    "    model_type = cfg.model._target_ if \"_target_\" in cfg.model else None\n",
    "\n",
    "    # Detectar cuál modelo estamos usando\n",
    "    if \"unet\" in cfg.model.__dict__['_content'] or \"unet\" in str(cfg.model):\n",
    "        print(\"📌 Instanciando modelo: U-Net Autoencoder\")\n",
    "        model = LitUNetAutoencoder(\n",
    "            model_cfg=cfg.model,\n",
    "            loss_cfg=cfg.loss,\n",
    "            optimizer_cfg=cfg.optimizer,\n",
    "        )\n",
    "    else:\n",
    "        print(\"📌 Instanciando modelo: Autoencoder clásico\")\n",
    "        model = LitAutoencoder(\n",
    "            model_cfg=cfg.model,\n",
    "            loss_cfg=cfg.loss,\n",
    "            optimizer_cfg=cfg.optimizer,\n",
    "            image_size=cfg.data.image_size,\n",
    "        )\n",
    "\n",
    "    # 4. WandB Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=cfg.logger.project,\n",
    "        entity=cfg.logger.entity,\n",
    "        log_model=cfg.logger.log_model,\n",
    "    )\n",
    "\n",
    "    # 5. Trainer de Lightning con parámetros de Hydra\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.trainer.max_epochs,\n",
    "        log_every_n_steps=cfg.trainer.log_every_n_steps,\n",
    "        deterministic=cfg.trainer.deterministic,\n",
    "        enable_model_summary=cfg.trainer.enable_model_summary,\n",
    "        enable_progress_bar=cfg.trainer.enable_progress_bar,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    # 6. Entrenamiento\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    print(\"✨ Entrenamiento finalizado correctamente ✨\")\n",
    "\n",
    "    return model, dm, cfg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
